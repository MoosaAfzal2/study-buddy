{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Study Buddy AI: Your Personalized Study Assistant**\n",
    "\n",
    "<!-- ##### **Description:**\n",
    "Study Buddy AI is an intelligent assistant designed to revolutionize the way students learn and plan their studies. Equipped with advanced features like personalized study plans, web search for research, math problem-solving, and human-in-the-loop assistance, it ensures accurate, actionable, and tailored support for academic success. Leveraging memory and contextual knowledge, Study Buddy AI creates dynamic, efficient schedules and provides step-by-step explanations to help students master any subject with ease. -->\n",
    "\n",
    "##### **Current Features:**\n",
    "\n",
    "- **Tool Assistance**: Integrates with tools like Study Scheduler and Web Search.\n",
    "- **Personalized Support**: Remembers user-specific details such as exam dates, preferred study times, and past topics to deliver customized responses.\n",
    "- **Study Schedule Creation**: Automatically generates optimized study plans based on deadlines, topics, and available time.\n",
    "- **User-Friendly Interaction**: Polite, clear, and concise responses with follow-up questions to ensure clarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEPENDENCIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U google-generativeai langchain-google-genai langgraph langchain langchain-core pydantic tavily-python langchain_community langgraph-checkpoint-sqlite chromadb langchain-text-splitters pypdf rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"GOOGLE_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "# _set_env(\"LANGCHAIN_API_KEY\")\n",
    "# _set_env(\"LANGCHAIN_PROJECT\")\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Annotated, Any, Dict, Union, Literal, Type\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import uuid\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END, add_messages\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition, InjectedState, InjectedStore\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain_core.tools import tool, InjectedToolArg\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    AnyMessage,\n",
    "    RemoveMessage,\n",
    "    ToolMessage,\n",
    "    AIMessageChunk,\n",
    ")\n",
    "\n",
    "from IPython.display import Image, clear_output, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would generally get the user_id from the authenticated User\n",
    "user_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_chat_template(\n",
    "    model: BaseChatModel,\n",
    "    template: str,\n",
    "    template_data: dict[str, Any],\n",
    ") -> BaseMessage:\n",
    "    \"\"\"Invoke the chat template with the provided tenplate data and return the response.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model\n",
    "    result = chain.invoke(template_data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def stream_agent(\n",
    "    agent: CompiledStateGraph,\n",
    "    input_value: Dict | List[Dict],\n",
    "    config: RunnableConfig,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    A function to interact with an agent in a streaming manner, handling updates and messages\n",
    "    during the stream. The function facilitates communication with the agent by providing\n",
    "    answers to questions when prompted and displaying messages based on specific conditions.\n",
    "\n",
    "    Args:\n",
    "        agent (CompiledStateGraph):\n",
    "            The compiled state graph representing the agent's behavior or knowledge graph.\n",
    "            This is used to drive the state transitions and handle the stream processing.\n",
    "\n",
    "        input_value (Dict | List[Dict]):\n",
    "            The input data that is passed to the agent. This can either be a single dictionary or\n",
    "            a list of dictionaries. These inputs are processed by the agent to generate stream outputs.\n",
    "\n",
    "        config (RunnableConfig):\n",
    "            The configuration object that governs how the agent runs. This may include settings\n",
    "            related to execution parameters, logging, error handling, and any other agent-specific configurations.\n",
    "\n",
    "        debug (bool):\n",
    "            If True, enables detailed logging for debugging purposes.\n",
    "\n",
    "    Returns:\n",
    "        None: This function is designed to interact with the agent and display messages interactively.\n",
    "              It doesn't return a value but may perform side effects such as displaying output.\n",
    "    \"\"\"\n",
    "    message = \"\"\n",
    "    for stream_mode, data in agent.stream(\n",
    "        input=input_value,\n",
    "        config=config,\n",
    "        stream_mode=[\"updates\", \"messages\"],\n",
    "        debug=debug,\n",
    "    ):\n",
    "        interrupt = (\n",
    "            data.get(\"__interrupt__\", None)\n",
    "            if isinstance(data, dict) and stream_mode == \"updates\"\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        if stream_mode == \"updates\" and interrupt and len(interrupt) > 0:\n",
    "            questions = interrupt[0].value\n",
    "            questions_with_ans = []\n",
    "            for question in questions:\n",
    "                user_input = input(question.get(\"question\", \"\"))\n",
    "\n",
    "                questions_with_ans.append({**question, \"answer\": user_input})\n",
    "\n",
    "            stream_agent(\n",
    "                agent=agent,\n",
    "                config=config,\n",
    "                input_value=Command(resume=questions_with_ans),\n",
    "            )\n",
    "\n",
    "        elif (\n",
    "            stream_mode == \"messages\"\n",
    "            and data[-1].get(\"langgraph_node\") == \"handle_user_query\"\n",
    "            and data[0].content\n",
    "            and isinstance(data[0], (AIMessage, AIMessageChunk))\n",
    "        ):\n",
    "            message += data[0].content  # Append content without a newline\n",
    "            clear_output(True)\n",
    "            display(Markdown(message))  # Display the message as Markdown\n",
    "\n",
    "\n",
    "def load_and_split_documents(\n",
    "    docs_folder: str,\n",
    "    file_extension: str,\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 50,\n",
    "    loader_cls=None,\n",
    "    loader_kwargs=None,\n",
    "    text_splitter_language=Language.MARKDOWN,\n",
    "    disable_splitter: bool = False,\n",
    "    metadata: dict = None,\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Load and split documents from a directory, based on file extension.\n",
    "\n",
    "    Args:\n",
    "    - docs_folder (str): Path to the folder containing documents.\n",
    "    - file_extension (str): The file extension to match (e.g., '.txt', '.pdf').\n",
    "    - chunk_size (int): The size of each chunk for text splitting.\n",
    "    - chunk_overlap (int): The overlap size for text splitting.\n",
    "    - loader_cls (class): The class to use for loading files (TextLoader or PyPDFLoader).\n",
    "    - loader_kwargs (dict): Additional arguments for the loader class.\n",
    "    - text_splitter_language (Language): The language setting for text splitting.\n",
    "    - disable_splitter (bool): For Disabling Splitter. defaults to False\n",
    "    - metadata (dict): Additional metadata for docs. defaults to None\n",
    "\n",
    "    Returns:\n",
    "    - List of split document chunks.\n",
    "    \"\"\"\n",
    "    if loader_kwargs is None:\n",
    "        loader_kwargs = {}\n",
    "\n",
    "    # Define a text splitter with the chosen language\n",
    "    text_splitter = (\n",
    "        RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "        if disable_splitter\n",
    "        else RecursiveCharacterTextSplitter.from_language(\n",
    "            language=text_splitter_language,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Initialize directory loader\n",
    "    file_loader = DirectoryLoader(\n",
    "        path=docs_folder,\n",
    "        glob=f\"**/*{file_extension}\",\n",
    "        show_progress=True,\n",
    "        silent_errors=True,\n",
    "        loader_cls=loader_cls,\n",
    "        loader_kwargs=loader_kwargs,\n",
    "    )\n",
    "\n",
    "    # Load and split documents\n",
    "    loaded_docs = file_loader.load_and_split(text_splitter)\n",
    "\n",
    "    docs = (\n",
    "        [\n",
    "            Document(\n",
    "                **doc.model_dump(exclude=\"metadata\"),\n",
    "                metadata={**doc.metadata, **metadata},\n",
    "            )\n",
    "            for doc in loaded_docs\n",
    "        ]\n",
    "        if metadata\n",
    "        else loaded_docs\n",
    "    )\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gemini-2.0-flash-exp\"\n",
    "# MODEL_NAME = \"gemini-1.5-flash\"\n",
    "EMBEDDING_MODEL_NAME = \"models/embedding-001\"\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0)\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a TypedDict for each file type configuration\n",
    "class FileConfig(TypedDict):\n",
    "    type: str  # File extension, e.g., \".txt\", \".pdf\"\n",
    "    loader: Type  # The loader class, e.g., TextLoader, PyPDFLoader\n",
    "    text_splitter_language: Language  # The language for the text splitter\n",
    "    loader_kwargs: dict  # Additional keyword arguments for the loader class\n",
    "    disable_splitter: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the file types and their respective loaders and text splitter languages\n",
    "supported_files_configs: List[FileConfig] = [\n",
    "    {\n",
    "        \"type\": \".txt\",\n",
    "        \"loader\": TextLoader,\n",
    "        \"text_splitter_language\": Language.MARKDOWN,\n",
    "        \"loader_kwargs\": {\"autodetect_encoding\": True},\n",
    "        \"metadata\": {\"user_id\": str(user_id)},\n",
    "    },\n",
    "    {\n",
    "        \"type\": \".md\",\n",
    "        \"loader\": TextLoader,\n",
    "        \"text_splitter_language\": Language.MARKDOWN,\n",
    "        \"loader_kwargs\": {\"autodetect_encoding\": True},\n",
    "        \"metadata\": {\"user_id\": str(user_id)},\n",
    "    },\n",
    "    {\n",
    "        \"type\": \".pdf\",\n",
    "        \"loader\": PyPDFLoader,\n",
    "        \"text_splitter_language\": Language.MARKDOWN,\n",
    "        \"loader_kwargs\": {\"extract_images\": True},\n",
    "        \"metadata\": {\"user_id\": str(user_id)},\n",
    "    },\n",
    "    {\n",
    "        \"type\": \".csv\",\n",
    "        \"loader\": CSVLoader,\n",
    "        \"disable_splitter\": True,\n",
    "        \"metadata\": {\"user_id\": str(user_id)},\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 39.45it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.41s/it]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Example of using the function for various file types (text and PDF):\n",
    "docs_folder = \"./docs\"\n",
    "docs = []\n",
    "\n",
    "# Loop through each file configuration and load the corresponding documents\n",
    "for config in supported_files_configs:\n",
    "    file_extension = config[\"type\"]\n",
    "    loader_cls = config[\"loader\"]\n",
    "    text_splitter_language = config.get(\"text_splitter_language\")\n",
    "    loader_kwargs = config.get(\"loader_kwargs\", {})\n",
    "    metadata = config.get(\"metadata\", None)\n",
    "    disable_splitter = config.get(\"disable_splitter\", None)\n",
    "\n",
    "    # Load and split documents for the current file type\n",
    "    docs += load_and_split_documents(\n",
    "        docs_folder=docs_folder,\n",
    "        file_extension=file_extension,\n",
    "        loader_cls=loader_cls,\n",
    "        loader_kwargs=loader_kwargs,\n",
    "        text_splitter_language=text_splitter_language,\n",
    "        disable_splitter=disable_splitter,\n",
    "        metadata=metadata,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector DB (Chroma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "vector_db_path = \"./vector_db\"\n",
    "# Create the directory if it doesn't exist\n",
    "Path(vector_db_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"vector-chroma-db\",\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=vector_db_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputState(TypedDict):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    response: AIMessage\n",
    "\n",
    "\n",
    "class OverallState(InputState, OutputState):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    summary: Optional[str] = None\n",
    "    memories: Optional[str] = None\n",
    "    conversation: Annotated[List[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERSISTANCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In memory\n",
    "conn = sqlite3.connect(\":memory:\", check_same_thread=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file db already exists.\n"
     ]
    }
   ],
   "source": [
    "# # pull file if it doesn't exist and connect to local db\n",
    "!mkdir db && [ ! -f db/sqlite.db ] && wget -P db https://github.com/MoosaAfzal2/study-buddy/db/sqlite.db && wget -P db https://github.com/MoosaAfzal2/study-buddy/db/sqlite.db-wal\n",
    "\n",
    "db_path = \"db/sqlite.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the Checkpointer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our checkpointer\n",
    "checkpointer: SqliteSaver = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMORY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Memory Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embedding_model,\n",
    "        \"dims\": 1536,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMESPACE_NAME = \"memories\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ProgressType(TypedDict):\n",
    "#     \"\"\"\n",
    "#     Defines the structure of progress for a study topic as a percentage.\n",
    "#     Each progress entry includes the topic name and its completion percentage.\n",
    "#     \"\"\"\n",
    "\n",
    "#     topic: Optional[str] = None  # The name of the study topic (e.g., \"Algebra\")\n",
    "#     progress_percentage: Optional[float] = (\n",
    "#         None  # The completion percentage for the topic (e.g., 75.0)\n",
    "#     )\n",
    "\n",
    "\n",
    "class MemoryType(BaseModel):\n",
    "    \"\"\"General memory structure for a Study Buddy Agent to store memory\"\"\"\n",
    "\n",
    "    content: str = Field(\n",
    "        description='The main content of the memory. For example: \"User expressed interest in learning about French.\"'\n",
    "    )\n",
    "\n",
    "    context: str = Field(\n",
    "        description='Additional context for the memory. For example: \"This was mentioned while discussing career options in Europe.\"'\n",
    "    )\n",
    "\n",
    "    # # User Information (Tracking user-specific details)\n",
    "    # username: Optional[str] = Field(\n",
    "    #     default=None, description='The username of the user (e.g., \"john_doe\")'\n",
    "    # )\n",
    "\n",
    "    # age: Optional[int] = Field(\n",
    "    #     default=None, description=\"The age of the user (e.g., 21)\"\n",
    "    # )\n",
    "\n",
    "    # gender: Optional[str] = Field(\n",
    "    #     default=None,\n",
    "    #     description='The gender of the user (e.g., \"Male\", \"Female\", \"Non-binary\")',\n",
    "    # )\n",
    "\n",
    "    # preferences: Optional[str] = Field(\n",
    "    #     default=None,\n",
    "    #     description=\"User preferences, such as preferred study time, study methods, etc.\",\n",
    "    # )\n",
    "\n",
    "    # # Study Preferences\n",
    "    # study_duration: Optional[int] = Field(\n",
    "    #     default=None,\n",
    "    #     description=\"Preferred study duration in minutes (e.g., 60 minutes)\",\n",
    "    # )\n",
    "\n",
    "    # break_duration: Optional[int] = Field(\n",
    "    #     default=None,\n",
    "    #     description=\"Preferred break duration in minutes (e.g., 15 minutes)\",\n",
    "    # )\n",
    "\n",
    "    # # Scheduling and Deadlines\n",
    "    # reminders: List[ReminderType] = Field(\n",
    "    #     default=[],\n",
    "    #     description='List of reminders with descriptions and dates (e.g., [{\"description\": \"Study for Math exam\", \"date\": \"2024-12-15\"}])',\n",
    "    # )\n",
    "\n",
    "    # deadlines: List[ReminderType] = Field(\n",
    "    #     default=[],\n",
    "    #     description='List of deadlines with event names and corresponding dates (e.g., [{\"description\": \"Assignment deadline\", \"date\": \"2024-12-18\"}])',\n",
    "    # )\n",
    "\n",
    "    # # Study Topics\n",
    "    # topics_to_study: List[str] = Field(\n",
    "    #     default=[], description='List of topics to study (e.g., [\"Algebra\", \"History\"])'\n",
    "    # )\n",
    "\n",
    "    # completed_topics: List[str] = Field(\n",
    "    #     default=[],\n",
    "    #     description='List of topics that have been completed (e.g., [\"Calculus\", \"Literature\"])',\n",
    "    # )\n",
    "\n",
    "    # # Progress Tracking\n",
    "    # progress: List[ProgressType] = Field(\n",
    "    #     default=[],\n",
    "    #     description='Progress for each study topic as a percentage (e.g., [{\"topic\": \"Algebra\", \"progress_percentage\": 75.0}])',\n",
    "    # )\n",
    "\n",
    "    # # Personal Notes and Goals\n",
    "    # personal_notes: Optional[str] = Field(\n",
    "    #     default=None,\n",
    "    #     description=\"Space for personal goals, motivational notes, or other reminders\",\n",
    "    # )\n",
    "\n",
    "    # # Last Updated Timestamp\n",
    "    # last_updated: Optional[str] = Field(\n",
    "    #     default=None,\n",
    "    #     description='Timestamp for when the memory was last updated (e.g., \"2024-12-15T14:30:00\")',\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOOLS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Search Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_search_tool = TavilySearchResults(\n",
    "    name=\"tavily_web_search\",\n",
    "    max_results=3,\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask Human Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Questions(BaseModel):\n",
    "    question: str\n",
    "    answer: Optional[str] = None\n",
    "\n",
    "\n",
    "@tool\n",
    "def ask_human(questions: List[Questions]):\n",
    "    \"\"\"\n",
    "    Prompts the user with a list of questions and captures their answers.\n",
    "\n",
    "    This function is designed to ask a series of questions to a human and store\n",
    "    their responses. Each question is represented by a dictionary that contains\n",
    "    an optional `id`, the `question` text, and an optional `answer`.\n",
    "\n",
    "    Args:\n",
    "        questions (List[Questions]): A list of question dictionaries. Each dictionary must contain the following keys:\n",
    "            - `question` (str): The question text to be presented to the user.\n",
    "            - `answer` (Optional[str], optional): The user's answer to the question. This is optional and may be left blank if the answer is unknown or pending. A default answer value may also be provided if desired.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of dictionaries representing the user's responses. Each dictionary will contain the `id` and `answer` for each question. The `answer` field will hold the user-provided response or remain empty if not provided.\n",
    "    \"\"\"\n",
    "\n",
    "    user_input = interrupt(\n",
    "        value=[\n",
    "            {\"id\": str(uuid.uuid4()), **question.model_dump()} for question in questions\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return user_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study Scheduling Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudySession(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a study session with details about the subject, time, notes.\n",
    "    \"\"\"\n",
    "\n",
    "    day: str = Field(\n",
    "        description='The day of the week for the study session (e.g., \"Monday\")'\n",
    "    )\n",
    "    subject: str = Field(description='The subject being studied (e.g., \"Math\")')\n",
    "    start_time: str = Field(\n",
    "        description='Start time of the session in \"HH:MM\" format (e.g., \"09:00\")'\n",
    "    )\n",
    "    end_time: str = Field(\n",
    "        description='End time of the session in \"HH:MM\" format (e.g., \"11:00\")'\n",
    "    )\n",
    "    notes: str = Field(\n",
    "        description='Additional notes for the session (e.g., \"Focus on Algebra\")'\n",
    "    )\n",
    "    priority: str = Field(\n",
    "        description='Priority of the study session (e.g., \"High\", \"Medium\", \"Low\")'\n",
    "    )\n",
    "    estimated_completion_time: int = Field(\n",
    "        description=\"Estimated time for completion of the subject in minutes\"\n",
    "    )\n",
    "    study_type: str = Field(\n",
    "        description='Type of study session (e.g., \"Focused\", \"Review\")'\n",
    "    )\n",
    "    status: str = Field(description='Completion status (e.g., \"Completed\", \"Pending\")')\n",
    "    study_notes: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description='Specific study notes for the session (e.g., \"Focus on chapters 3-5\")',\n",
    "    )\n",
    "    available_time_per_day: int = Field(\n",
    "        description=\"Total study time available per day in minutes\"\n",
    "    )\n",
    "\n",
    "\n",
    "@tool\n",
    "def create_study_schedule(\n",
    "    study_sessions: List[StudySession],  # List of StudySession dictionaries\n",
    ") -> List[StudySession]:\n",
    "    \"\"\"\n",
    "    Creates a personalized study schedule based on user preferences, subjects, and available time.\n",
    "\n",
    "    This function takes a list of study sessions, calculates the required time for each, and creates a study\n",
    "    schedule based on the available time per day. It also ensures that the number of sessions does not exceed\n",
    "    the available time, provides a break between consecutive study sessions, and includes warnings when the\n",
    "    available time is insufficient for all sessions.\n",
    "\n",
    "    Args:\n",
    "        study_sessions (List[StudySession]): A list of dictionaries, each representing a study session. Each dictionary must contain the following keys:\n",
    "            - 'day': The day of the week for the study session (e.g., \"Monday\").\n",
    "            - 'subject': The subject being studied (e.g., \"Math\").\n",
    "            - 'start_time': The start time of the session in \"HH:MM\" format.\n",
    "            - 'end_time': The end time of the session in \"HH:MM\" format.\n",
    "            - 'notes': Additional notes for the session (e.g., \"Focus on Algebra\").\n",
    "            - 'priority': The priority of the session (e.g., \"High\", \"Medium\", \"Low\").\n",
    "            - 'estimated_completion_time': The estimated duration for completing the session in minutes.\n",
    "            - 'study_type': The type of session (e.g., \"Focused\", \"Review\").\n",
    "            - 'status': The completion status of the session (e.g., \"Completed\", \"Pending\").\n",
    "            - 'study_notes': Specific study notes for the session (optional).\n",
    "            - 'available_time_per_day': The total study time available per day in minutes.\n",
    "\n",
    "    Returns:\n",
    "        List[StudySession]: A list of dictionaries representing the generated study schedule. Each dictionary contains the following keys:\n",
    "            - 'day': The day of the week for the study session.\n",
    "            - 'subject': The subject being studied.\n",
    "            - 'start_time': The start time of the session in \"HH:MM\" format.\n",
    "            - 'end_time': The end time of the session in \"HH:MM\" format.\n",
    "            - 'notes': Additional notes for the session.\n",
    "            - 'priority': The priority of the session.\n",
    "            - 'estimated_completion_time': The estimated duration for completing the session.\n",
    "            - 'study_type': The type of session.\n",
    "            - 'status': The completion status of the session.\n",
    "            - 'study_notes': Specific study notes for the session.\n",
    "            - 'available_time_per_day': The total study time available per day.\n",
    "            - Breaks: A break is included after each session unless it’s the last session.\n",
    "\n",
    "    Warnings:\n",
    "        - If the total time for all sessions exceeds the available time per day, the function will try to fit as many sessions as possible.\n",
    "        - If there is remaining time after scheduling, a message will be shown to inform the user.\n",
    "    \"\"\"\n",
    "\n",
    "    schedule: List = []\n",
    "    current_day = datetime.now()\n",
    "\n",
    "    for session in study_sessions:\n",
    "        # Parse start_time into a datetime object\n",
    "        start_time = datetime.strptime(session.start_time, \"%H:%M\").replace(\n",
    "            year=current_day.year, month=current_day.month, day=current_day.day\n",
    "        )\n",
    "\n",
    "        # Adjust start time if it has already passed today\n",
    "        if start_time < current_day:\n",
    "            start_time += timedelta(days=1)  # Schedule for the next day\n",
    "\n",
    "        # Calculate end_time based on estimated duration\n",
    "        end_time = start_time + timedelta(minutes=session.estimated_completion_time)\n",
    "\n",
    "        # Create and add the study session to the schedule\n",
    "        new_session = StudySession(\n",
    "            day=start_time.strftime(\"%A\"),\n",
    "            subject=session.subject,\n",
    "            start_time=start_time.strftime(\"%H:%M\"),\n",
    "            end_time=end_time.strftime(\"%H:%M\"),\n",
    "            notes=session.notes,\n",
    "            priority=session.priority,\n",
    "            estimated_completion_time=session.estimated_completion_time,\n",
    "            study_type=session.study_type,\n",
    "            status=session.status,\n",
    "            study_notes=session.study_notes or \"\",  # Use empty string if None\n",
    "            available_time_per_day=session.available_time_per_day,\n",
    "        )\n",
    "\n",
    "        schedule.append(new_session.model_dump())\n",
    "\n",
    "        # Add a break time after each session (unless it's the last session)\n",
    "        if session != study_sessions[-1]:\n",
    "            break_time_start = end_time\n",
    "            break_time_end = break_time_start + timedelta(minutes=15)\n",
    "            break_session = StudySession(\n",
    "                day=start_time.strftime(\"%A\"),\n",
    "                subject=\"Break\",\n",
    "                start_time=break_time_start.strftime(\"%H:%M\"),\n",
    "                end_time=break_time_end.strftime(\"%H:%M\"),\n",
    "                notes=\"Take a 15-minute break\",\n",
    "                priority=\"Low\",\n",
    "                estimated_completion_time=15,\n",
    "                study_type=\"Break\",\n",
    "                status=\"Pending\",\n",
    "                study_notes=\"\",\n",
    "                available_time_per_day=session.available_time_per_day,\n",
    "            )\n",
    "\n",
    "            schedule.append(break_session.model_dump())\n",
    "\n",
    "    return schedule\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_study_schedule_to_markdown(\n",
    "    markdown: str,\n",
    "    file_name: str = \"study_schedule.md\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Saves the study schedule content into a Markdown file, ensuring proper line breaks and formatting.\n",
    "\n",
    "    Args:\n",
    "        markdown (str): The content in Markdown format to be saved.\n",
    "        file_name (str): The name of the Markdown file where the schedule will be saved (default is 'study_schedule.md').\n",
    "\n",
    "    Returns:\n",
    "        str: Success Messages\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure line breaks are properly handled and formatting is correct\n",
    "        formatted_markdown = markdown.replace(\"\\\\n\", \"\\n\")  # Ensure proper line breaks\n",
    "\n",
    "        # Open the file in write mode and save the Markdown content\n",
    "        with open(\"./docs/\" + file_name, mode=\"w\") as file:\n",
    "            file.write(formatted_markdown)\n",
    "        return f\"Study schedule saved successfully to {file_name}\"\n",
    "\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied when trying to write to {file_name}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Directory or file path {file_name} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving study schedule: {e}\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_study_schedule_to_csv(\n",
    "    schedule_data: List[StudySession],\n",
    "    file_name: str = \"study_schedule.csv\",\n",
    "    field_names: List[str] = [\n",
    "        \"day\",\n",
    "        \"subject\",\n",
    "        \"start_time\",\n",
    "        \"end_time\",\n",
    "        \"notes\",\n",
    "        \"priority\",\n",
    "        \"estimated_completion_time\",\n",
    "        \"study_type\",\n",
    "        \"status\",\n",
    "        \"study_notes\",\n",
    "        \"available_time_per_day\",\n",
    "    ],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Saves the study schedule to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        schedule_data (List[StudySession]): A list of dictionaries, each representing a study session. Each dictionary must contain the following keys:\n",
    "            - 'day': The day of the week for the study session (e.g., \"Monday\").\n",
    "            - 'subject': The subject being studied (e.g., \"Math\").\n",
    "            - 'start_time': The start time of the session in \"HH:MM\" format.\n",
    "            - 'end_time': The end time of the session in \"HH:MM\" format.\n",
    "            - 'notes': Additional notes for the session (e.g., \"Focus on Algebra\").\n",
    "            - 'priority': The priority of the session (e.g., \"High\", \"Medium\", \"Low\").\n",
    "            - 'estimated_completion_time': The estimated duration for completing the session in minutes.\n",
    "            - 'study_type': The type of session (e.g., \"Focused\", \"Review\").\n",
    "            - 'status': The completion status of the session (e.g., \"Completed\", \"Pending\").\n",
    "            - 'study_notes': Specific study notes for the session (optional).\n",
    "            - 'available_time_per_day': The total study time available per day in minutes.\n",
    "        file_name (str): The name of the CSV file where the schedule will be saved (default is 'study_schedule.csv').\n",
    "        field_names (List[str]): The column names for the CSV file (default is [\"day\",\"subject\",\"start_time\",\"end_time\",\"notes\",\"priority\",\"estimated_completion_time\",\"study_type\",\"status\",\"study_notes\",\"available_time_per_day\"]).\n",
    "\n",
    "    Returns:\n",
    "        str: Success Messages\n",
    "    \"\"\"\n",
    "\n",
    "    # Dynamically fetch required keys from StudySession TypedDict\n",
    "    required_keys = set(StudySession.__annotations__.keys())  # Set of required keys\n",
    "\n",
    "    for session in schedule_data:\n",
    "        if not required_keys.issubset(session.model_dump().keys()):\n",
    "            raise ValueError(\n",
    "                f\"Each schedule entry must contain the following keys: {', '.join(required_keys)}\"\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        with open(\"./docs/\" + file_name, mode=\"w\", newline=\"\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=field_names)\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Handling falsy or empty values for any field in the session\n",
    "            for session in schedule_data:\n",
    "                session_dict = (\n",
    "                    session.model_copy().model_dump()\n",
    "                )  # Create a copy of the session to avoid modifying the original\n",
    "\n",
    "                # Iterate through each field and handle falsy or empty values\n",
    "                for key, value in session_dict.items():\n",
    "                    if value in (None, \"\", [], {}, 0, False):  # Check for falsy values\n",
    "                        if key == \"study_notes\":\n",
    "                            session_dict[key] = (\n",
    "                                \"\"  # Empty study_notes should be an empty string\n",
    "                            )\n",
    "                        else:\n",
    "                            session_dict[key] = (\n",
    "                                \"N/A\"  # Default for other fields (e.g., \"N/A\" or 0)\n",
    "                            )\n",
    "\n",
    "                # Write the session row\n",
    "                writer.writerow(session_dict)\n",
    "\n",
    "        return f\"Study schedule saved successfully to {file_name}\"\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied when trying to write to {file_name}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_name} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving study schedule: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def upsert_memory(\n",
    "    memory: MemoryType,\n",
    "    *,\n",
    "    memory_id: Optional[Union[str, uuid.UUID]] = None,\n",
    "    # Hide these arguments from the model.\n",
    "    config: Annotated[RunnableConfig, InjectedToolArg],\n",
    "    store: Annotated[BaseStore, InjectedStore],\n",
    "):\n",
    "    \"\"\"Upsert a memory in the database.\n",
    "\n",
    "    If a memory conflicts with an existing one, then just UPDATE the\n",
    "    existing one by passing in memory_id - don't create two memories\n",
    "    that are the same. If the user corrects a memory, UPDATE it.\n",
    "\n",
    "    Args:\n",
    "        memory (MemoryType): A memory dictionary containing the following keys:\n",
    "            - `content`: The main content of the memory. For example: \"User expressed interest in learning about French.\"\n",
    "            - `context`: Additional context for the memory. For example: \"This was mentioned while discussing career options in Europe.\"\n",
    "        memory_id: ONLY PROVIDE IF UPDATING AN EXISTING MEMORY. The memory to overwrite.\n",
    "    \"\"\"\n",
    "    mem_id = memory_id or uuid.uuid4()\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    store.put((NAMESPACE_NAME, user_id), key=str(mem_id), value=memory.model_dump())\n",
    "    return f\"Stored memory {mem_id}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def delete_memory(\n",
    "    memory_id: uuid.UUID | str,\n",
    "    config: RunnableConfig,\n",
    "    store: Annotated[BaseStore, InjectedToolArg],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Delete a memory from the database by its ID.\n",
    "    This function removes a specific memory from the store based on the provided\n",
    "    memory ID. The memory ID must exist in the store for the deletion to be successful.\n",
    "\n",
    "    Args:\n",
    "        memory_id (uuid.UUID | str): The ID of the memory to delete.\n",
    "\n",
    "    Returns:\n",
    "        str: A message confirming whether the memory was successfully deleted or if the memory ID was not found.\n",
    "    \"\"\"\n",
    "\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    memory_key = str(memory_id)\n",
    "\n",
    "    # Check if the memory exists before deleting\n",
    "    memory = store.get((NAMESPACE_NAME, user_id), key=memory_key)\n",
    "\n",
    "    if memory:\n",
    "        store.delete((NAMESPACE_NAME, user_id), key=memory_key)\n",
    "        return f\"Memory {memory_id} successfully deleted.\"\n",
    "    else:\n",
    "        return f\"Memory {memory_id} not found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchKwargs(BaseModel):\n",
    "    search_type: Optional[str] = Field(\n",
    "        default=\"similarity\",\n",
    "        description=(\n",
    "            \"Defines the type of search the Retriever should perform. \"\n",
    "            'Can be \"similarity\" (default), \"mmr\", or \"similarity_score_threshold\".'\n",
    "        ),\n",
    "    )\n",
    "    k: Optional[int] = Field(\n",
    "        default=4, description=\"Number of documents to return (Default: 4)\"\n",
    "    )\n",
    "    score_threshold: Optional[float] = Field(\n",
    "        default=None,\n",
    "        description=\"Minimum relevance threshold for similarity_score_threshold.\",\n",
    "    )\n",
    "    fetch_k: Optional[int] = Field(\n",
    "        default=20,\n",
    "        description=\"Number of documents to pass to MMR algorithm (Default: 20).\",\n",
    "    )\n",
    "    lambda_mult: Optional[float] = Field(\n",
    "        default=0.5,\n",
    "        description=(\n",
    "            \"Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum (Default: 0.5).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class RetrieveDocumentsArgs(BaseModel):\n",
    "    query: str\n",
    "    kwargs: SearchKwargs\n",
    "    config: Annotated[RunnableConfig, InjectedToolArg] = Field(None)\n",
    "\n",
    "\n",
    "@tool(args_schema=RetrieveDocumentsArgs)\n",
    "def retrieve_documents(\n",
    "    query: str,\n",
    "    kwargs: SearchKwargs,\n",
    "    *,\n",
    "    config: Annotated[RunnableConfig, InjectedToolArg] = Field(None),\n",
    "):\n",
    "    \"\"\"\n",
    "    Search and return user documents.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string.\n",
    "        kwargs (Optional[SearchKwargs]): dictionary of arguments to pass to the search function with keys:\n",
    "            - `search_type` (Optional[str]): Defines the type of search that the Retriever should perform. Can be \"similarity\" (default), \"mmr\", or \"similarity_score_threshold\".\n",
    "            - `k` (Optional[int]): Amount of documents to return (Default: 4)\n",
    "            - `score_threshold` (Optional[float]): Minimum relevance threshold for similarity_score_threshold\n",
    "            - `fetch_k` (Optional[int]): Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "            - `lambda_mult` (Optional[float]): Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "    \"\"\"\n",
    "    # Extract user_id from config\n",
    "    user_id = str(config.get(\"configurable\", {}).get(\"user_id\", \"\"))\n",
    "    if not user_id:\n",
    "        raise ValueError(\"Missing user_id in configuration.\")\n",
    "\n",
    "    # Prepare search parameters\n",
    "    search_kwargs = kwargs.model_dump(include=[\"k\"])\n",
    "    search_kwargs[\"filter\"] = {\"user_id\": user_id}\n",
    "\n",
    "    if kwargs.search_type == \"similarity_score_threshold\":\n",
    "        search_kwargs[\"score_threshold\"] = kwargs.score_threshold\n",
    "    elif kwargs.search_type == \"mmr\":\n",
    "        search_kwargs.update(\n",
    "            {\"fetch_k\": kwargs.fetch_k, \"lambda_mult\": kwargs.lambda_mult}\n",
    "        )\n",
    "\n",
    "    # Initialize the retriever with search parameters\n",
    "    retriever = vector_db.as_retriever(\n",
    "        search_type=kwargs.search_type, search_kwargs=search_kwargs\n",
    "    )\n",
    "    try:\n",
    "        retrieved_documents = retriever.invoke(input=query, config=config)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during document retrieval: {str(e)}\")\n",
    "\n",
    "    formatted_docs = \"\"\n",
    "    for document in retrieved_documents:\n",
    "        page_no = document.metadata.get(\"page\")  # Fetch the page number\n",
    "        source = document.metadata.get(\"source\", \"\")\n",
    "        content = document.page_content\n",
    "\n",
    "        # Append metadata and content to docs\n",
    "        if page_no is not None:  # Check if page_no exists and is not None\n",
    "            formatted_docs += f\"Page: {page_no}\\n\\n\"\n",
    "        if source:\n",
    "            formatted_docs += f\"Source: {source}\\n\\n\"\n",
    "        formatted_docs += f\"Content: {content}\\n\\n\"\n",
    "\n",
    "\n",
    "    return formatted_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Available Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    tavily_search_tool,\n",
    "    ask_human,\n",
    "    create_study_schedule,\n",
    "    save_study_schedule_to_csv,\n",
    "    save_study_schedule_to_markdown,\n",
    "    upsert_memory,\n",
    "    delete_memory,\n",
    "    retrieve_documents,\n",
    "]\n",
    "\n",
    "model_with_tools = model.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NODES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Node\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_node(\n",
    "    state: OverallState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore,\n",
    ") -> OverallState:\n",
    "    \"\"\"Retrieve memories associated with a user based on a query.\"\"\"\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "\n",
    "    memories = store.search(\n",
    "        (NAMESPACE_NAME, user_id),\n",
    "        query=str([m.content for m in state[\"messages\"][-3:]]),\n",
    "        limit=10,\n",
    "    )\n",
    "\n",
    "    # Format memories for inclusion in the prompt\n",
    "    formatted_memories = \"\\n\".join(\n",
    "        f\"[{mem.key}]: {mem.value} (similarity: {mem.score})\" for mem in memories\n",
    "    )\n",
    "\n",
    "    if formatted_memories:\n",
    "        formatted_memories = f\"\"\"\n",
    "<memories>\n",
    "{formatted_memories}\n",
    "</memories>\n",
    "\"\"\"\n",
    "\n",
    "    return {\"memories\": formatted_memories}\n",
    "\n",
    "\n",
    "class ParsedMemory(BaseModel):\n",
    "    memory_id: Optional[str] = None\n",
    "    memory: MemoryType\n",
    "\n",
    "\n",
    "def update_memory_node(\n",
    "    state: OverallState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore,\n",
    ") -> OverallState:\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    memories = state.get(\"memories\", \"\")\n",
    "    user_query = state.get(\"query\", \"\")\n",
    "    history = summary if summary else state.get(\"messages\", [])\n",
    "\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    namespace = (NAMESPACE_NAME, user_id)\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=ParsedMemory)\n",
    "\n",
    "    # Create our memories prompt\n",
    "    memories_message = ChatPromptTemplate.from_template(\n",
    "        \"This is the current user's memory namespace: {namespace}\\n\\n\"\n",
    "        \"This is summary or history of the conversation to date: {history}\\n\\n\"\n",
    "        \"This is the latest user query: {user_query}\\n\\n\"\n",
    "        \"These are the memories of the user based on the latest query: {memories}\\n\\n\"\n",
    "        \"If a memory conflicts with an existing one, then just UPDATE the existing one by passing in memory_id - don't create two memories that are the same.\"\n",
    "        \"Now based on the above if you think that the current memories should be updated or a new memory should be added then return the new or updated memory in the following Format:\"\n",
    "        \"\"\"\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "        \"EXAMPLES:\"\n",
    "        \" - Creating a New Memory:\"\n",
    "        \"\"\"\n",
    "        {{\n",
    "          \"memory\": {{\n",
    "            \"content\": \"User expressed interest in learning about French.\",\n",
    "            \"context\": \"This was mentioned while discussing career options in Europe.\",\n",
    "            ...other_fields\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \" - Updating a Memory:\"\n",
    "        \"\"\"\n",
    "        {{\n",
    "          \"memory_id\": \"...\",\n",
    "          \"memory\": {{\n",
    "            \"content\": \"User expressed interest in learning about French.\",\n",
    "            \"context\": \"This was mentioned while discussing career options in Europe.\",\n",
    "            ...other_fields\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "    ).format(\n",
    "        format_instructions=parser.get_format_instructions(),\n",
    "        history=history,\n",
    "        memories=memories,\n",
    "        user_query=user_query,\n",
    "        namespace=str(namespace),\n",
    "    )\n",
    "\n",
    "    response = model.invoke(memories_message)\n",
    "\n",
    "    parsed_response = parser.parse(response.content)\n",
    "\n",
    "    mem_id = parsed_response.memory_id or uuid.uuid4()\n",
    "    mem_type = parsed_response.memory_type or \"general\"\n",
    "    store.put(\n",
    "        namespace,\n",
    "        key=str(mem_id),\n",
    "        value={  # Store memory with its type\n",
    "            \"memory_type\": str(mem_type),\n",
    "            \"memory_data\": parsed_response.model_dump().get(\"memory\"),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return {\"memories\": state.get(\"memories\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_node(state: InputState) -> OverallState:\n",
    "    user_query = state.get(\"query\")\n",
    "    # Return the result as a response\n",
    "    return {\"messages\": [HumanMessage(content=user_query)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle User Query Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are Study Buddy AI, an intelligent assistant designed to support students with their study plans, research, and problem-solving tasks. Your core capabilities include:\n",
    "\n",
    "1. **Tool Assistance**: Utilize external tools such as Study Scheduler, math solvers, calendar APIs, Web Search, and knowledge bases to provide accurate, actionable, and tailored solutions.\n",
    "2. **Web Search**: Use external tools like Tavily Web Search to find relevant reference materials for the user.\n",
    "3. **Human-in-the-Loop (HITL)**: Whenever you need to get human input or clarification, always use the **ask human** tool. Make sure to gather all necessary details in a single request to avoid multiple interactions. Avoid assumptions and ask for precise, unambiguous information.\n",
    "4. **Personalized Support**: Use memory to provide tailored responses, such as recalling exam dates, preferred study times, and previous topics covered.\n",
    "5. **Agentic RAG (Retrieval-Augmented Generation)**: Retrieve contextually relevant information from knowledge bases, user Documents using Tools to generate accurate, concise answers.\n",
    "\n",
    "### BEHAVIORAL GUIDELINES:\n",
    "- Be polite, clear, and concise in your responses.\n",
    "- Ensure accuracy and ask for clarification if needed to avoid misunderstandings.\n",
    "- Provide detailed explanations, especially when it comes to study concepts or solving problems.\n",
    "- Always provide educational insights to help the user understand, not just the answer.\n",
    "- When planning or scheduling, ask about topics to cover, time availability, and any constraints or preferences related to the study routine using the **ask human** tool.\n",
    "- If the user has no preferences or details, create an optimal study schedule based on best practices.\n",
    "- For ambiguous or complex queries, **pause and use the ask human tool** to involve a human assistant.\n",
    "\n",
    "### REQUIREMENTS:\n",
    "- ALWAYS FIRST RETRIEVE DOCUMENTS USING TOOLS.\n",
    "- If there is missing or unclear information (e.g., study preferences, specific topics, or scheduling constraints), you must **ask human** to clarify.\n",
    "- If you need to gather multiple pieces of information, **ask human** should include all necessary details in one message.\n",
    "- ALWAYS USE THE **ask human** to take any kind of input or information from the user and always ask precise questions to avoid multiple rounds of clarification.\n",
    "- If you have already asked for user input and you still need more information you can invoke the ask human tool again.\n",
    "- WHEN DOING RAG, If the tool response is not relevant to the user's query, try to retreive the documents again using tools.\n",
    "\n",
    "### EXAMPLES:\n",
    "\n",
    "#### Human: \"What is Langgraph?\"\n",
    "- **Tool Call**: Use tools to search the web and gather content.\n",
    "- **AI**: \"Langgraph is an AI framework designed to...\"\n",
    "\n",
    "#### Human: \"Please create a study schedule for me.\"\n",
    "- **Tool Call (ask human)**: \n",
    "  - AI: \"In order to create a personalized study schedule for you, I'll need a few details. Please answer the following:\n",
    "         **Asks the user all the details needed to create a schedule.**\n",
    "- **AI (after receiving user input)**: \n",
    "  - **Tool Call**: calls the create study schedule to create schedule.\n",
    "  - AI: \"Here is your Study schedule...\"\n",
    "\n",
    "#### Human: \"Please save that schedule in a (markdown|csv) file.\"\n",
    "- **Tool Call**: Save the study schedule in (markdown|csv) format.\n",
    "- **AI (using tool message)**: \"I have successfully saved your study schedule in (markdown|csv) format. Let me know if you need any further adjustments.\"\n",
    "\n",
    "#### Human: \"What is my name?\"\n",
    "- **Tool Call**: Tries to search in documents.\n",
    "- **AI (If answer was not found in docs)**: \"I'm sorry, but I don’t know your name yet. Could you please share it with me so I can address you properly in future conversations?\"\n",
    "\n",
    "### MEMORIES\n",
    "{memories}\n",
    "\n",
    "### SUMMARY\n",
    "{summary}\n",
    "\n",
    "System Time: {time}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def handle_user_query_node(state: OverallState) -> OverallState:\n",
    "    \"\"\"\n",
    "    Calls the model with the correct input state, integrating all messages (including tool calls and prior responses)\n",
    "    to ensure the model has the full context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the conversation history and user query\n",
    "    messages = state.get(\"messages\", [])\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    memories = state.get(\"memories\", \"\")\n",
    "\n",
    "    # Construct the system message with the summary and memories (if available)\n",
    "    system_message = SystemMessage(\n",
    "        content=system_prompt.format(\n",
    "            summary=summary,  # Include summary context if available\n",
    "            memories=memories,  # Include memory context if available\n",
    "            time=datetime.now().isoformat(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Combine system message, prior tool messages\n",
    "    # Ensure all relevant messages are included in the context\n",
    "    all_messages = [system_message] + messages\n",
    "\n",
    "    # Call the model with the full message history (system message, tool messages, and user query)\n",
    "    result = model_with_tools.invoke(all_messages)\n",
    "\n",
    "    if result.tool_calls:\n",
    "        result = AIMessage(**result.model_dump(exclude=[\"content\"]), content=\"\")\n",
    "\n",
    "    # Return the result as a response\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarization Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization_node(state: OverallState) -> OverallState:\n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\")\n",
    "    messages = state.get(\"messages\")\n",
    "\n",
    "    # Create our summarization prompt\n",
    "    if summary:\n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    if len(messages) > 6:\n",
    "        # Add prompt to our history\n",
    "        messages = messages + [HumanMessage(content=summary_message)]\n",
    "        response = model.invoke(messages)\n",
    "        # Delete all but the 2 most recent messages\n",
    "        delete_messages = [RemoveMessage(id=m.id or \"\") for m in messages[:-2]]\n",
    "\n",
    "        return {\n",
    "            \"summary\": response.content,\n",
    "            \"messages\": delete_messages,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        return {\"summary\": \"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_node(state: OverallState) -> OutputState:\n",
    "    # If the graph is successful we add the query and response to full conversation history\n",
    "    conversation = []\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.type == \"ai\" or last_message.type == \"AIMessageChunk\":\n",
    "        conversation += [HumanMessage(content=state[\"query\"])] + [last_message]\n",
    "\n",
    "    return {\n",
    "        \"response\": state[\"messages\"][-1],\n",
    "        \"conversation\": conversation,\n",
    "        \"query\": \"\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT COMPILATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "workflow = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"input\", input_node)\n",
    "workflow.add_node(\"handle_user_query\", handle_user_query_node)\n",
    "workflow.add_node(\"get_memory\", get_memory_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"summarization\", summarization_node)\n",
    "workflow.add_node(\"finalize_output\", output_node)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"input\")\n",
    "workflow.add_edge(\"input\", \"get_memory\")\n",
    "workflow.add_edge(\"get_memory\", \"handle_user_query\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_user_query\", tools_condition, {\"tools\": \"tools\", END: \"summarization\"}\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"handle_user_query\")\n",
    "workflow.add_edge(\"summarization\", \"finalize_output\")\n",
    "workflow.add_edge(\"finalize_output\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile(checkpointer=checkpointer, store=in_memory_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 989,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(agent.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING SCENARIOIS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEED REFERENCE MATERIALS FOR LEARNING?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"10\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, I am trying to Learn Langgraph, can you please find some reference material from the web for me to get started?', additional_kwargs={}, response_metadata={}, id='1455802d-bfcc-4e26-8d5d-0f668ba25524'),\n",
       " AIMessage(content=\"Okay, I found some helpful resources to get you started with Langgraph:\\n\\n1.  **LangGraph Quickstart**: This tutorial from the official Langchain documentation guides you through building a chatbot that uses tools and tracks conversation history. It also covers human-in-the-loop capabilities and time-travel features. You can find it here: [https://langchain-ai.github.io/langgraph/tutorials/](https://langchain-ai.github.io/langgraph/tutorials/)\\n2.  **Introduction to LangGraph: A Beginner's Guide**: This article provides a good overview of LangGraph, explaining its core concepts and how to use it to build complex applications with LLMs. Check it out here: [https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141](https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141)\\n3.  **Getting Started with LangGraph: A Practical Guide for Beginners**: This blog post dives into the basics of LangGraph, explaining what it is, why it's useful, and how to set it up. You can read it here: [https://toxigon.com/getting-started-with-langgraph](https://toxigon.com/getting-started-with-langgraph)\\n\\nThese resources should give you a solid foundation for learning Langgraph. Let me know if you need help with anything else!\\n\", additional_kwargs={}, response_metadata={'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}], 'finish_reason': 'STOP'}, id='run-9e095014-cbe4-4b05-adea-b6964cd48935', usage_metadata={'input_tokens': 4637, 'output_tokens': 334, 'total_tokens': 4971, 'input_token_details': {'cache_read': 0}}),\n",
       " ToolMessage(content='[{\"url\": \"https://langchain-ai.github.io/langgraph/tutorials/\", \"content\": \"Tutorials¶. New to LangGraph or LLM app development? Read this material to get up and running building your first applications. Get Started 🚀¶. LangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history.Add human-in-the-loop capabilities and explore how time-travel works.\"}, {\"url\": \"https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141\", \"content\": \"LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its\"}, {\"url\": \"https://toxigon.com/getting-started-with-langgraph\", \"content\": \"Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners What is LangGraph? Why Use LangGraph? Setting Up LangGraph In this article, we\\'ll dive into the basics of LangGraph, understand what it is, why it\\'s useful, and how to get started with it. What is LangGraph? Why Use LangGraph? Setting Up LangGraph The first step is to set up LangGraph. @article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} } How to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\"}]', name='tavily_web_search', id='a31c7560-1480-4830-b29c-ba745333f8c6', tool_call_id='a5460e23-72ef-4340-8426-6a9fbb0eba85', artifact={'query': 'Langgraph tutorial for beginners', 'follow_up_questions': None, 'answer': 'LangGraph offers tutorials for beginners to help them get started with developing applications using LangGraph. These tutorials cover topics such as building a chatbot, utilizing tools, keeping track of conversation history, adding human-in-the-loop capabilities, and exploring time-travel functionalities. Beginners can leverage LangGraph to build complex, stateful applications by understanding its core concepts and working through simple examples. Additionally, a practical guide is available for beginners to set up LangGraph, understand its usefulness, and begin working with it.', 'images': ['https://cdn.hashnode.com/res/hashnode/image/upload/v1727784426242/554ed438-b8fa-4721-b687-f7389ffbe04c.png?w=1600&h=840&fit=crop&crop=entropy&auto=compress,format&format=webp', 'https://cdn.hashnode.com/res/hashnode/image/upload/v1727695448677/0e3ce539-1997-43ef-8e3c-648893855349.jpeg?auto=compress,format&format=webp', 'https://cdn.hashnode.com/res/hashnode/image/upload/v1727691758803/9a7b226b-8372-4580-8847-ab8c5fac09d0.webp?auto=compress,format&format=webp', 'https://datasciencedojo.com/wp-content/uploads/LinkedIn-Newsletr-Infographics.png', 'https://miro.medium.com/v2/resize:fit:1200/1*pvsQBbW2fTYhvYcQIMCt3w.png'], 'results': [{'title': 'Tutorials - GitHub Pages', 'url': 'https://langchain-ai.github.io/langgraph/tutorials/', 'content': 'Tutorials¶. New to LangGraph or LLM app development? Read this material to get up and running building your first applications. Get Started 🚀¶. LangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history.Add human-in-the-loop capabilities and explore how time-travel works.', 'score': 0.8058924, 'raw_content': None}, {'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\", 'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141', 'content': 'LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its', 'score': 0.8051581, 'raw_content': 'Sign up\\nSign in\\nSign up\\nSign in\\nIntroduction to LangGraph: A Beginner’s Guide\\nCPlog\\nFollow\\n--\\n2\\nListen\\nShare\\nLangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, we’ll introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\\nWhat is LangGraph?\\nLangGraph is a library built on top of LangChain, designed to add cyclic computational capabilities to your LLM applications. While LangChain allows you to define chains of computation (Directed Acyclic Graphs or DAGs), LangGraph introduces the ability to add cycles, enabling more complex, agent-like behaviors where you can call an LLM in a loop, asking it what action to take next.\\nKey Concepts\\nA Simple Example\\nLet’s walk through a simple example where we use LangGraph to classify user input as either a “greeting” or a “search” query and respond accordingly.\\nStep 1: Define the Graph State\\nFirst, we define the state structure for our graph. In this example, our state includes the user’s question, the classification of the question, and a response.\\nStep 2: Create the Graph\\nNext, we create a new instance of StateGraph with our GraphState structure.\\nStep 3: Define Nodes\\nWe define nodes for classifying the input, handling greetings, and handling search queries.\\nStep 4: Add Nodes to the Graph\\nWe add our nodes to the graph and define the flow using edges and conditional edges.\\nStep 5: Set Entry and End Points\\nWe set the entry point for our graph and define the end points.\\nStep 6: Compile and Run the Graph\\nFinally, we compile our graph and run it with some input.\\nCommon Confusions\\nConclusion\\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding!\\n--\\n--\\n2\\nWritten by CPlog\\nAI enthusiast & Principal Data Scientist at LFX & Li & Fung. I innovate with LLMs, LangChain, Stable Diffusion & ComfyUI to revolutionize supply chains.\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams'}, {'title': 'Getting Started with LangGraph: A Practical Guide for Beginners', 'url': 'https://toxigon.com/getting-started-with-langgraph', 'content': \"Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners What is LangGraph? Why Use LangGraph? Setting Up LangGraph In this article, we'll dive into the basics of LangGraph, understand what it is, why it's useful, and how to get started with it. What is LangGraph? Why Use LangGraph? Setting Up LangGraph The first step is to set up LangGraph. @article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} } How to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\", 'score': 0.77650476, 'raw_content': \"Getting Started with LangGraph: A Practical Guide for Beginners\\n ### TOXIGON Infinite\\nSearch\\n\\nHome\\n\\nProgramming 1 day ago 1\\n\\nGetting Started with LangGraph: A Practical Guide for Beginners\\nTable of Contents\\n\\nGetting Started with LangGraph: A Practical Guide for Beginners\\nWhat is LangGraph?\\nWhy Use LangGraph?\\nSetting Up LangGraph\\n\\nLangGraph is one of those tools that you've probably heard about, but maybe you're not sure where to start. Don't worry, I'm here to help. In this article, we'll dive into the basics of LangGraph, understand what it is, why it's useful, and how to get started with it.\\nBy the end of this guide, you're going to have a solid foundation in LangGraph. You'll understand what it's all about, how to set it up, and even create your first graph. So, let's dive in!\\nWhat is LangGraph?\\nLangGraph is a powerful tool designed to help you visualize and analyze language data. It's particularly useful for linguists, data scientists, and anyone interested in understanding the structure and relationships within text data.\\nWhy Use LangGraph?\\nBefore we dive into the how, let's talk about the why. Why should you bother learning LangGraph? Well, here are a few reasons:\\n\\nVisualize Data: LangGraph allows you to create visual representations of your data, making it easier to spot patterns and relationships.\\nAnalyze Text: It's great for analyzing text data, helping you understand the structure and semantics of language.\\nFlexible: LangGraph is highly customizable, allowing you to tailor it to your specific needs.\\n\\nSounds pretty useful, right?\\nSetting Up LangGraph\\nAlright, let's get into the nitty-gritty. The first step is to set up LangGraph. Here's a quick rundown of what you'll need:\\n\\nA computer (obviously)\\nPython installed (you can download it from \\n\\nCitation\\n@article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} }\\nRelated Articles\\n\\nDiving Deep into Test-Driven Development in Go: A Practical Guide\\nDiving Deep into Test-Driven Development in Go: A Practical Guide Welcome, fellow developers! Today,...\\n4 days ago 0\\n\\nHow to Get Started with Spring Framework: A Comprehensive Overview\\nHow to Get Started with Spring Framework: A Comprehensive Overview Welcome to our comprehensive gui...\\n2 months ago 14\\n\\nHow to Boost Performance with Parallel Processing in .NET\\nHow to Boost Performance with Parallel Processing in .NET Welcome to the world of parallel processi...\\n2 months ago 0\\n\\nHow to Install .NET Core on Linux: A Step-by-Step Guide\\nHow to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\\n4 months ago 21\\n© 2004 - 2024 Toxigon Infinite.\\nAll rights reserved.\"}], 'response_time': 2.91}),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'name': 'tavily_web_search', 'arguments': '{\"query\": \"Langgraph tutorial for beginners\"}'}}, response_metadata={'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}], 'finish_reason': 'STOP'}, id='run-14762d44-7e9b-468c-9c60-9155e1da4c9c', tool_calls=[{'name': 'tavily_web_search', 'args': {'query': 'Langgraph tutorial for beginners'}, 'id': 'a5460e23-72ef-4340-8426-6a9fbb0eba85', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4195, 'output_tokens': 12, 'total_tokens': 4207, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='Hi, I am trying to Learn Langgraph, can you please find some reference material from the web for me to get started?', additional_kwargs={}, response_metadata={}, id='474eff28-ceb6-416c-8e2a-2eca2563c25d')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content='[{\"url\": \"https://langchain-ai.github.io/langgraph/tutorials/\", \"content\": \"Tutorials¶. New to LangGraph or LLM app development? Read this material to get up and running building your first applications. Get Started 🚀¶. LangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history.Add human-in-the-loop capabilities and explore how time-travel works.\"}, {\"url\": \"https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141\", \"content\": \"LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its\"}, {\"url\": \"https://toxigon.com/getting-started-with-langgraph\", \"content\": \"Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners What is LangGraph? Why Use LangGraph? Setting Up LangGraph In this article, we\\'ll dive into the basics of LangGraph, understand what it is, why it\\'s useful, and how to get started with it. What is LangGraph? Why Use LangGraph? Setting Up LangGraph The first step is to set up LangGraph. @article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} } How to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\"}]', name='tavily_web_search', id='96c64cff-8beb-47f7-a9dd-999a717acb8b', tool_call_id='3fe99dde-cb3a-435e-bd48-735c3689e10c', artifact={'query': 'Langgraph tutorial for beginners', 'follow_up_questions': None, 'answer': 'LangGraph offers tutorials for beginners to help them get started with building applications using LangGraph and LLM technology. Beginners can learn to create chatbots, incorporate human-in-the-loop capabilities, and understand how time-travel functions within LangGraph. Additionally, beginners can leverage LangGraph to build complex, stateful applications by grasping its core concepts and working through simple examples. The tool provides a practical guide for beginners to understand its basics, utility, and setup process.', 'images': ['https://cdn.hashnode.com/res/hashnode/image/upload/v1727784426242/554ed438-b8fa-4721-b687-f7389ffbe04c.png?w=1600&h=840&fit=crop&crop=entropy&auto=compress,format&format=webp', 'https://cdn.hashnode.com/res/hashnode/image/upload/v1727695448677/0e3ce539-1997-43ef-8e3c-648893855349.jpeg?auto=compress,format&format=webp', 'https://cdn.hashnode.com/res/hashnode/image/upload/v1727691758803/9a7b226b-8372-4580-8847-ab8c5fac09d0.webp?auto=compress,format&format=webp', 'https://datasciencedojo.com/wp-content/uploads/LinkedIn-Newsletr-Infographics.png', 'https://miro.medium.com/v2/resize:fit:1200/1*pvsQBbW2fTYhvYcQIMCt3w.png'], 'results': [{'title': 'Tutorials - GitHub Pages', 'url': 'https://langchain-ai.github.io/langgraph/tutorials/', 'content': 'Tutorials¶. New to LangGraph or LLM app development? Read this material to get up and running building your first applications. Get Started 🚀¶. LangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history.Add human-in-the-loop capabilities and explore how time-travel works.', 'score': 0.8058924, 'raw_content': None}, {'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\", 'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141', 'content': 'LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its', 'score': 0.8051581, 'raw_content': 'Sign up\\nSign in\\nSign up\\nSign in\\nIntroduction to LangGraph: A Beginner’s Guide\\nCPlog\\nFollow\\n--\\n2\\nListen\\nShare\\nLangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, we’ll introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\\nWhat is LangGraph?\\nLangGraph is a library built on top of LangChain, designed to add cyclic computational capabilities to your LLM applications. While LangChain allows you to define chains of computation (Directed Acyclic Graphs or DAGs), LangGraph introduces the ability to add cycles, enabling more complex, agent-like behaviors where you can call an LLM in a loop, asking it what action to take next.\\nKey Concepts\\nA Simple Example\\nLet’s walk through a simple example where we use LangGraph to classify user input as either a “greeting” or a “search” query and respond accordingly.\\nStep 1: Define the Graph State\\nFirst, we define the state structure for our graph. In this example, our state includes the user’s question, the classification of the question, and a response.\\nStep 2: Create the Graph\\nNext, we create a new instance of StateGraph with our GraphState structure.\\nStep 3: Define Nodes\\nWe define nodes for classifying the input, handling greetings, and handling search queries.\\nStep 4: Add Nodes to the Graph\\nWe add our nodes to the graph and define the flow using edges and conditional edges.\\nStep 5: Set Entry and End Points\\nWe set the entry point for our graph and define the end points.\\nStep 6: Compile and Run the Graph\\nFinally, we compile our graph and run it with some input.\\nCommon Confusions\\nConclusion\\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding!\\n--\\n--\\n2\\nWritten by CPlog\\nAI enthusiast & Principal Data Scientist at LFX & Li & Fung. I innovate with LLMs, LangChain, Stable Diffusion & ComfyUI to revolutionize supply chains.\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams'}, {'title': 'Getting Started with LangGraph: A Practical Guide for Beginners', 'url': 'https://toxigon.com/getting-started-with-langgraph', 'content': \"Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners What is LangGraph? Why Use LangGraph? Setting Up LangGraph In this article, we'll dive into the basics of LangGraph, understand what it is, why it's useful, and how to get started with it. What is LangGraph? Why Use LangGraph? Setting Up LangGraph The first step is to set up LangGraph. @article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} } How to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\", 'score': 0.77650476, 'raw_content': \"Getting Started with LangGraph: A Practical Guide for Beginners\\n ### TOXIGON Infinite\\nSearch\\n\\nHome\\n\\nProgramming 1 day ago 1\\n\\nGetting Started with LangGraph: A Practical Guide for Beginners\\nTable of Contents\\n\\nGetting Started with LangGraph: A Practical Guide for Beginners\\nWhat is LangGraph?\\nWhy Use LangGraph?\\nSetting Up LangGraph\\n\\nLangGraph is one of those tools that you've probably heard about, but maybe you're not sure where to start. Don't worry, I'm here to help. In this article, we'll dive into the basics of LangGraph, understand what it is, why it's useful, and how to get started with it.\\nBy the end of this guide, you're going to have a solid foundation in LangGraph. You'll understand what it's all about, how to set it up, and even create your first graph. So, let's dive in!\\nWhat is LangGraph?\\nLangGraph is a powerful tool designed to help you visualize and analyze language data. It's particularly useful for linguists, data scientists, and anyone interested in understanding the structure and relationships within text data.\\nWhy Use LangGraph?\\nBefore we dive into the how, let's talk about the why. Why should you bother learning LangGraph? Well, here are a few reasons:\\n\\nVisualize Data: LangGraph allows you to create visual representations of your data, making it easier to spot patterns and relationships.\\nAnalyze Text: It's great for analyzing text data, helping you understand the structure and semantics of language.\\nFlexible: LangGraph is highly customizable, allowing you to tailor it to your specific needs.\\n\\nSounds pretty useful, right?\\nSetting Up LangGraph\\nAlright, let's get into the nitty-gritty. The first step is to set up LangGraph. Here's a quick rundown of what you'll need:\\n\\nA computer (obviously)\\nPython installed (you can download it from \\n\\nCitation\\n@article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} }\\nRelated Articles\\n\\nDiving Deep into Test-Driven Development in Go: A Practical Guide\\nDiving Deep into Test-Driven Development in Go: A Practical Guide Welcome, fellow developers! Today,...\\n4 days ago 0\\n\\nHow to Get Started with Spring Framework: A Comprehensive Overview\\nHow to Get Started with Spring Framework: A Comprehensive Overview Welcome to our comprehensive gui...\\n2 months ago 14\\n\\nHow to Boost Performance with Parallel Processing in .NET\\nHow to Boost Performance with Parallel Processing in .NET Welcome to the world of parallel processi...\\n2 months ago 0\\n\\nHow to Install .NET Core on Linux: A Step-by-Step Guide\\nHow to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\\n4 months ago 21\\n© 2004 - 2024 Toxigon Infinite.\\nAll rights reserved.\"}], 'response_time': 2.96}),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'name': 'tavily_web_search', 'arguments': '{\"query\": \"Langgraph tutorial for beginners\"}'}}, response_metadata={'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}], 'finish_reason': 'STOP'}, id='run-54c2d878-5efc-45c1-beba-e638b6e4a2ac', tool_calls=[{'name': 'tavily_web_search', 'args': {'query': 'Langgraph tutorial for beginners'}, 'id': '3fe99dde-cb3a-435e-bd48-735c3689e10c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4226, 'output_tokens': 12, 'total_tokens': 4238, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='Hi, I am trying to Learn Langgraph, can you please find some reference material from the web for me to get started?', additional_kwargs={}, response_metadata={}, id='1455802d-bfcc-4e26-8d5d-0f668ba25524'),\n",
       " AIMessage(content=\"Okay, I found some helpful resources to get you started with Langgraph:\\n\\n1.  **LangGraph Quickstart**: This tutorial from the official Langchain documentation guides you through building a chatbot that uses tools and tracks conversation history. It also covers human-in-the-loop capabilities and time-travel features. You can find it here: [https://langchain-ai.github.io/langgraph/tutorials/](https://langchain-ai.github.io/langgraph/tutorials/)\\n2.  **Introduction to LangGraph: A Beginner's Guide**: This article provides a good overview of LangGraph, explaining its core concepts and how to use it to build complex applications with LLMs. Check it out here: [https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141](https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141)\\n3.  **Getting Started with LangGraph: A Practical Guide for Beginners**: This blog post dives into the basics of LangGraph, explaining what it is, why it's useful, and how to set it up. You can read it here: [https://toxigon.com/getting-started-with-langgraph](https://toxigon.com/getting-started-with-langgraph)\\n\\nThese resources should give you a solid foundation for learning Langgraph. Let me know if you need help with anything else!\\n\", additional_kwargs={}, response_metadata={'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}], 'finish_reason': 'STOP'}, id='run-9e095014-cbe4-4b05-adea-b6964cd48935', usage_metadata={'input_tokens': 4637, 'output_tokens': 334, 'total_tokens': 4971, 'input_token_details': {'cache_read': 0}}),\n",
       " ToolMessage(content='[{\"url\": \"https://langchain-ai.github.io/langgraph/tutorials/\", \"content\": \"Tutorials¶. New to LangGraph or LLM app development? Read this material to get up and running building your first applications. Get Started 🚀¶. LangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history.Add human-in-the-loop capabilities and explore how time-travel works.\"}, {\"url\": \"https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141\", \"content\": \"LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its\"}, {\"url\": \"https://toxigon.com/getting-started-with-langgraph\", \"content\": \"Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners What is LangGraph? Why Use LangGraph? Setting Up LangGraph In this article, we\\'ll dive into the basics of LangGraph, understand what it is, why it\\'s useful, and how to get started with it. What is LangGraph? Why Use LangGraph? Setting Up LangGraph The first step is to set up LangGraph. @article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} } How to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\"}]', name='tavily_web_search', id='a31c7560-1480-4830-b29c-ba745333f8c6', tool_call_id='a5460e23-72ef-4340-8426-6a9fbb0eba85', artifact={'query': 'Langgraph tutorial for beginners', 'follow_up_questions': None, 'answer': 'LangGraph offers tutorials for beginners to help them get started with developing applications using LangGraph. These tutorials cover topics such as building a chatbot, utilizing tools, keeping track of conversation history, adding human-in-the-loop capabilities, and exploring time-travel functionalities. Beginners can leverage LangGraph to build complex, stateful applications by understanding its core concepts and working through simple examples. Additionally, a practical guide is available for beginners to set up LangGraph, understand its usefulness, and begin working with it.', 'images': ['https://cdn.hashnode.com/res/hashnode/image/upload/v1727784426242/554ed438-b8fa-4721-b687-f7389ffbe04c.png?w=1600&h=840&fit=crop&crop=entropy&auto=compress,format&format=webp', 'https://cdn.hashnode.com/res/hashnode/image/upload/v1727695448677/0e3ce539-1997-43ef-8e3c-648893855349.jpeg?auto=compress,format&format=webp', 'https://cdn.hashnode.com/res/hashnode/image/upload/v1727691758803/9a7b226b-8372-4580-8847-ab8c5fac09d0.webp?auto=compress,format&format=webp', 'https://datasciencedojo.com/wp-content/uploads/LinkedIn-Newsletr-Infographics.png', 'https://miro.medium.com/v2/resize:fit:1200/1*pvsQBbW2fTYhvYcQIMCt3w.png'], 'results': [{'title': 'Tutorials - GitHub Pages', 'url': 'https://langchain-ai.github.io/langgraph/tutorials/', 'content': 'Tutorials¶. New to LangGraph or LLM app development? Read this material to get up and running building your first applications. Get Started 🚀¶. LangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history.Add human-in-the-loop capabilities and explore how time-travel works.', 'score': 0.8058924, 'raw_content': None}, {'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\", 'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141', 'content': 'LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its', 'score': 0.8051581, 'raw_content': 'Sign up\\nSign in\\nSign up\\nSign in\\nIntroduction to LangGraph: A Beginner’s Guide\\nCPlog\\nFollow\\n--\\n2\\nListen\\nShare\\nLangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, we’ll introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\\nWhat is LangGraph?\\nLangGraph is a library built on top of LangChain, designed to add cyclic computational capabilities to your LLM applications. While LangChain allows you to define chains of computation (Directed Acyclic Graphs or DAGs), LangGraph introduces the ability to add cycles, enabling more complex, agent-like behaviors where you can call an LLM in a loop, asking it what action to take next.\\nKey Concepts\\nA Simple Example\\nLet’s walk through a simple example where we use LangGraph to classify user input as either a “greeting” or a “search” query and respond accordingly.\\nStep 1: Define the Graph State\\nFirst, we define the state structure for our graph. In this example, our state includes the user’s question, the classification of the question, and a response.\\nStep 2: Create the Graph\\nNext, we create a new instance of StateGraph with our GraphState structure.\\nStep 3: Define Nodes\\nWe define nodes for classifying the input, handling greetings, and handling search queries.\\nStep 4: Add Nodes to the Graph\\nWe add our nodes to the graph and define the flow using edges and conditional edges.\\nStep 5: Set Entry and End Points\\nWe set the entry point for our graph and define the end points.\\nStep 6: Compile and Run the Graph\\nFinally, we compile our graph and run it with some input.\\nCommon Confusions\\nConclusion\\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding!\\n--\\n--\\n2\\nWritten by CPlog\\nAI enthusiast & Principal Data Scientist at LFX & Li & Fung. I innovate with LLMs, LangChain, Stable Diffusion & ComfyUI to revolutionize supply chains.\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams'}, {'title': 'Getting Started with LangGraph: A Practical Guide for Beginners', 'url': 'https://toxigon.com/getting-started-with-langgraph', 'content': \"Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners What is LangGraph? Why Use LangGraph? Setting Up LangGraph In this article, we'll dive into the basics of LangGraph, understand what it is, why it's useful, and how to get started with it. What is LangGraph? Why Use LangGraph? Setting Up LangGraph The first step is to set up LangGraph. @article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} } How to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\", 'score': 0.77650476, 'raw_content': \"Getting Started with LangGraph: A Practical Guide for Beginners\\n ### TOXIGON Infinite\\nSearch\\n\\nHome\\n\\nProgramming 1 day ago 1\\n\\nGetting Started with LangGraph: A Practical Guide for Beginners\\nTable of Contents\\n\\nGetting Started with LangGraph: A Practical Guide for Beginners\\nWhat is LangGraph?\\nWhy Use LangGraph?\\nSetting Up LangGraph\\n\\nLangGraph is one of those tools that you've probably heard about, but maybe you're not sure where to start. Don't worry, I'm here to help. In this article, we'll dive into the basics of LangGraph, understand what it is, why it's useful, and how to get started with it.\\nBy the end of this guide, you're going to have a solid foundation in LangGraph. You'll understand what it's all about, how to set it up, and even create your first graph. So, let's dive in!\\nWhat is LangGraph?\\nLangGraph is a powerful tool designed to help you visualize and analyze language data. It's particularly useful for linguists, data scientists, and anyone interested in understanding the structure and relationships within text data.\\nWhy Use LangGraph?\\nBefore we dive into the how, let's talk about the why. Why should you bother learning LangGraph? Well, here are a few reasons:\\n\\nVisualize Data: LangGraph allows you to create visual representations of your data, making it easier to spot patterns and relationships.\\nAnalyze Text: It's great for analyzing text data, helping you understand the structure and semantics of language.\\nFlexible: LangGraph is highly customizable, allowing you to tailor it to your specific needs.\\n\\nSounds pretty useful, right?\\nSetting Up LangGraph\\nAlright, let's get into the nitty-gritty. The first step is to set up LangGraph. Here's a quick rundown of what you'll need:\\n\\nA computer (obviously)\\nPython installed (you can download it from \\n\\nCitation\\n@article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} }\\nRelated Articles\\n\\nDiving Deep into Test-Driven Development in Go: A Practical Guide\\nDiving Deep into Test-Driven Development in Go: A Practical Guide Welcome, fellow developers! Today,...\\n4 days ago 0\\n\\nHow to Get Started with Spring Framework: A Comprehensive Overview\\nHow to Get Started with Spring Framework: A Comprehensive Overview Welcome to our comprehensive gui...\\n2 months ago 14\\n\\nHow to Boost Performance with Parallel Processing in .NET\\nHow to Boost Performance with Parallel Processing in .NET Welcome to the world of parallel processi...\\n2 months ago 0\\n\\nHow to Install .NET Core on Linux: A Step-by-Step Guide\\nHow to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\\n4 months ago 21\\n© 2004 - 2024 Toxigon Infinite.\\nAll rights reserved.\"}], 'response_time': 2.91}),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'name': 'tavily_web_search', 'arguments': '{\"query\": \"Langgraph tutorial for beginners\"}'}}, response_metadata={'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}], 'finish_reason': 'STOP'}, id='run-14762d44-7e9b-468c-9c60-9155e1da4c9c', tool_calls=[{'name': 'tavily_web_search', 'args': {'query': 'Langgraph tutorial for beginners'}, 'id': 'a5460e23-72ef-4340-8426-6a9fbb0eba85', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4195, 'output_tokens': 12, 'total_tokens': 4207, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='Hi, I am trying to Learn Langgraph, can you please find some reference material from the web for me to get started?', additional_kwargs={}, response_metadata={}, id='474eff28-ceb6-416c-8e2a-2eca2563c25d')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content='[{\"url\": \"https://langchain-ai.github.io/langgraph/tutorials/\", \"content\": \"Tutorials¶. New to LangGraph or LLM app development? Read this material to get up and running building your first applications. Get Started 🚀¶. LangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history.Add human-in-the-loop capabilities and explore how time-travel works.\"}, {\"url\": \"https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141\", \"content\": \"LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its\"}, {\"url\": \"https://toxigon.com/getting-started-with-langgraph\", \"content\": \"Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners What is LangGraph? Why Use LangGraph? Setting Up LangGraph In this article, we\\'ll dive into the basics of LangGraph, understand what it is, why it\\'s useful, and how to get started with it. What is LangGraph? Why Use LangGraph? Setting Up LangGraph The first step is to set up LangGraph. @article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} } How to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\"}]', name='tavily_web_search', id='96c64cff-8beb-47f7-a9dd-999a717acb8b', tool_call_id='3fe99dde-cb3a-435e-bd48-735c3689e10c', artifact={'query': 'Langgraph tutorial for beginners', 'follow_up_questions': None, 'answer': 'LangGraph offers tutorials for beginners to help them get started with building applications using LangGraph and LLM technology. Beginners can learn to create chatbots, incorporate human-in-the-loop capabilities, and understand how time-travel functions within LangGraph. Additionally, beginners can leverage LangGraph to build complex, stateful applications by grasping its core concepts and working through simple examples. The tool provides a practical guide for beginners to understand its basics, utility, and setup process.', 'images': ['https://cdn.hashnode.com/res/hashnode/image/upload/v1727784426242/554ed438-b8fa-4721-b687-f7389ffbe04c.png?w=1600&h=840&fit=crop&crop=entropy&auto=compress,format&format=webp', 'https://cdn.hashnode.com/res/hashnode/image/upload/v1727695448677/0e3ce539-1997-43ef-8e3c-648893855349.jpeg?auto=compress,format&format=webp', 'https://cdn.hashnode.com/res/hashnode/image/upload/v1727691758803/9a7b226b-8372-4580-8847-ab8c5fac09d0.webp?auto=compress,format&format=webp', 'https://datasciencedojo.com/wp-content/uploads/LinkedIn-Newsletr-Infographics.png', 'https://miro.medium.com/v2/resize:fit:1200/1*pvsQBbW2fTYhvYcQIMCt3w.png'], 'results': [{'title': 'Tutorials - GitHub Pages', 'url': 'https://langchain-ai.github.io/langgraph/tutorials/', 'content': 'Tutorials¶. New to LangGraph or LLM app development? Read this material to get up and running building your first applications. Get Started 🚀¶. LangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history.Add human-in-the-loop capabilities and explore how time-travel works.', 'score': 0.8058924, 'raw_content': None}, {'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\", 'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141', 'content': 'LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its', 'score': 0.8051581, 'raw_content': 'Sign up\\nSign in\\nSign up\\nSign in\\nIntroduction to LangGraph: A Beginner’s Guide\\nCPlog\\nFollow\\n--\\n2\\nListen\\nShare\\nLangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, we’ll introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\\nWhat is LangGraph?\\nLangGraph is a library built on top of LangChain, designed to add cyclic computational capabilities to your LLM applications. While LangChain allows you to define chains of computation (Directed Acyclic Graphs or DAGs), LangGraph introduces the ability to add cycles, enabling more complex, agent-like behaviors where you can call an LLM in a loop, asking it what action to take next.\\nKey Concepts\\nA Simple Example\\nLet’s walk through a simple example where we use LangGraph to classify user input as either a “greeting” or a “search” query and respond accordingly.\\nStep 1: Define the Graph State\\nFirst, we define the state structure for our graph. In this example, our state includes the user’s question, the classification of the question, and a response.\\nStep 2: Create the Graph\\nNext, we create a new instance of StateGraph with our GraphState structure.\\nStep 3: Define Nodes\\nWe define nodes for classifying the input, handling greetings, and handling search queries.\\nStep 4: Add Nodes to the Graph\\nWe add our nodes to the graph and define the flow using edges and conditional edges.\\nStep 5: Set Entry and End Points\\nWe set the entry point for our graph and define the end points.\\nStep 6: Compile and Run the Graph\\nFinally, we compile our graph and run it with some input.\\nCommon Confusions\\nConclusion\\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding!\\n--\\n--\\n2\\nWritten by CPlog\\nAI enthusiast & Principal Data Scientist at LFX & Li & Fung. I innovate with LLMs, LangChain, Stable Diffusion & ComfyUI to revolutionize supply chains.\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams'}, {'title': 'Getting Started with LangGraph: A Practical Guide for Beginners', 'url': 'https://toxigon.com/getting-started-with-langgraph', 'content': \"Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners Getting Started with LangGraph: A Practical Guide for Beginners What is LangGraph? Why Use LangGraph? Setting Up LangGraph In this article, we'll dive into the basics of LangGraph, understand what it is, why it's useful, and how to get started with it. What is LangGraph? Why Use LangGraph? Setting Up LangGraph The first step is to set up LangGraph. @article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} } How to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\", 'score': 0.77650476, 'raw_content': \"Getting Started with LangGraph: A Practical Guide for Beginners\\n ### TOXIGON Infinite\\nSearch\\n\\nHome\\n\\nProgramming 1 day ago 1\\n\\nGetting Started with LangGraph: A Practical Guide for Beginners\\nTable of Contents\\n\\nGetting Started with LangGraph: A Practical Guide for Beginners\\nWhat is LangGraph?\\nWhy Use LangGraph?\\nSetting Up LangGraph\\n\\nLangGraph is one of those tools that you've probably heard about, but maybe you're not sure where to start. Don't worry, I'm here to help. In this article, we'll dive into the basics of LangGraph, understand what it is, why it's useful, and how to get started with it.\\nBy the end of this guide, you're going to have a solid foundation in LangGraph. You'll understand what it's all about, how to set it up, and even create your first graph. So, let's dive in!\\nWhat is LangGraph?\\nLangGraph is a powerful tool designed to help you visualize and analyze language data. It's particularly useful for linguists, data scientists, and anyone interested in understanding the structure and relationships within text data.\\nWhy Use LangGraph?\\nBefore we dive into the how, let's talk about the why. Why should you bother learning LangGraph? Well, here are a few reasons:\\n\\nVisualize Data: LangGraph allows you to create visual representations of your data, making it easier to spot patterns and relationships.\\nAnalyze Text: It's great for analyzing text data, helping you understand the structure and semantics of language.\\nFlexible: LangGraph is highly customizable, allowing you to tailor it to your specific needs.\\n\\nSounds pretty useful, right?\\nSetting Up LangGraph\\nAlright, let's get into the nitty-gritty. The first step is to set up LangGraph. Here's a quick rundown of what you'll need:\\n\\nA computer (obviously)\\nPython installed (you can download it from \\n\\nCitation\\n@article{getting-started-with-langgraph, \\xa0\\xa0title = {Getting Started with LangGraph: A Practical Guide for Beginners}, \\xa0\\xa0author = {Toxigon}, \\xa0\\xa0year = {2024}, \\xa0\\xa0journal = {Toxigon Blog}, \\xa0\\xa0url = {https://toxigon.com/getting-started-with-langgraph} }\\nRelated Articles\\n\\nDiving Deep into Test-Driven Development in Go: A Practical Guide\\nDiving Deep into Test-Driven Development in Go: A Practical Guide Welcome, fellow developers! Today,...\\n4 days ago 0\\n\\nHow to Get Started with Spring Framework: A Comprehensive Overview\\nHow to Get Started with Spring Framework: A Comprehensive Overview Welcome to our comprehensive gui...\\n2 months ago 14\\n\\nHow to Boost Performance with Parallel Processing in .NET\\nHow to Boost Performance with Parallel Processing in .NET Welcome to the world of parallel processi...\\n2 months ago 0\\n\\nHow to Install .NET Core on Linux: A Step-by-Step Guide\\nHow to Install .NET Core on Linux: A Comprehensive Guide If you’re diving into the world of progra...\\n4 months ago 21\\n© 2004 - 2024 Toxigon Infinite.\\nAll rights reserved.\"}], 'response_time': 2.96})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 Unable to submit request because it must have a text parameter. Add a text parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1134\u001b[0m, in \u001b[0;36mGenerativeServiceClient.stream_generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 1134\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[0;32m   1135\u001b[0m     request,\n\u001b[0;32m   1136\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[0;32m   1137\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1138\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m   1139\u001b[0m )\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    294\u001b[0m     target,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    296\u001b[0m     sleep_generator,\n\u001b[0;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     _retry_error_helper(\n\u001b[0;32m    154\u001b[0m         exc,\n\u001b[0;32m    155\u001b[0m         deadline,\n\u001b[0;32m    156\u001b[0m         sleep,\n\u001b[0;32m    157\u001b[0m         error_list,\n\u001b[0;32m    158\u001b[0m         predicate,\n\u001b[0;32m    159\u001b[0m         on_error,\n\u001b[0;32m    160\u001b[0m         exception_factory,\n\u001b[0;32m    161\u001b[0m         timeout,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:174\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 Unable to submit request because it must have a text parameter. Add a text parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[970], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stream_agent(\n\u001b[0;32m      2\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m      3\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m      4\u001b[0m     input_value\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, I am trying to Learn Langgraph, can you please find some reference material from the web for me to get started?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m     },\n\u001b[0;32m      7\u001b[0m )\n",
      "Cell \u001b[1;32mIn[431], line 45\u001b[0m, in \u001b[0;36mstream_agent\u001b[1;34m(agent, input_value, config, debug)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mA function to interact with an agent in a streaming manner, handling updates and messages\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03mduring the stream. The function facilitates communication with the agent by providing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m          It doesn't return a value but may perform side effects such as displaying output.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stream_mode, data \u001b[38;5;129;01min\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39minput_value,\n\u001b[0;32m     47\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     48\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     49\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m     50\u001b[0m ):\n\u001b[0;32m     51\u001b[0m     interrupt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     52\u001b[0m         data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__interrupt__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m interrupt \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(interrupt) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1656\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1657\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   1658\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   1659\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   1660\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   1661\u001b[0m         ):\n\u001b[0;32m   1662\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\runner.py:239\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    237\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m _panic_or_proceed(\n\u001b[0;32m    240\u001b[0m     done_futures\u001b[38;5;241m.\u001b[39munion(f \u001b[38;5;28;01mfor\u001b[39;00m f, t \u001b[38;5;129;01min\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    241\u001b[0m     panic\u001b[38;5;241m=\u001b[39mreraise,\n\u001b[0;32m    242\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\runner.py:539\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m panic:\n\u001b[1;32m--> 539\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\executor.py:76\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         task\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[963], line 133\u001b[0m, in \u001b[0;36mhandle_user_query_node\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m    121\u001b[0m all_messages \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    122\u001b[0m     [system_message]\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;241m+\u001b[39m ([messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mand\u001b[39;00m messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [])  \u001b[38;5;66;03m# Last message if human\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;241m+\u001b[39m ([ai_msg] \u001b[38;5;28;01mif\u001b[39;00m ai_msg \u001b[38;5;28;01melse\u001b[39;00m [])  \u001b[38;5;66;03m# AI message if exists\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;241m+\u001b[39m order_tools  \u001b[38;5;66;03m# Sorted tool messages\u001b[39;00m\n\u001b[0;32m    126\u001b[0m )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Display all messages (optional, for debugging purposes)\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# display(ai_msg)\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# display(msgs)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Call the model with the full message history (system message, tool messages, and user query)\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m result \u001b[38;5;241m=\u001b[39m model_with_tools\u001b[38;5;241m.\u001b[39minvoke(all_messages)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtool_calls:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Safeguard for creating a clean AIMessage if tool calls are present\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     result \u001b[38;5;241m=\u001b[39m AIMessage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult\u001b[38;5;241m.\u001b[39mmodel_dump(exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]), content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5355\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5356\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5358\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    634\u001b[0m                 m,\n\u001b[0;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:839\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream(\n\u001b[0;32m    834\u001b[0m     async_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    835\u001b[0m     run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    837\u001b[0m ):\n\u001b[0;32m    838\u001b[0m     chunks: \u001b[38;5;28mlist\u001b[39m[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 839\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    840\u001b[0m         chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mresponse_metadata \u001b[38;5;241m=\u001b[39m _gen_info_and_msg_metadata(chunk)\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1029\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._stream\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream\u001b[39m(\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1005\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ChatGenerationChunk]:\n\u001b[0;32m   1018\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m   1019\u001b[0m         messages,\n\u001b[0;32m   1020\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m   1028\u001b[0m     )\n\u001b[1;32m-> 1029\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m   1030\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   1031\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mstream_generate_content,\n\u001b[0;32m   1032\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1033\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m   1034\u001b[0m     )\n\u001b[0;32m   1036\u001b[0m     prev_usage_metadata: UsageMetadata \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m action(retry_state)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: rs\u001b[38;5;241m.\u001b[39moutcome\u001b[38;5;241m.\u001b[39mresult())\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:190\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m: Invalid argument provided to Gemini: 400 Unable to submit request because it must have a text parameter. Add a text parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini",
      "\u001b[0mDuring task with name 'handle_user_query' and id 'd4c3d391-0901-fb77-ee9b-d085cab469e6'"
     ]
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\n",
    "        \"query\": \"Hi, I am trying to Learn Langgraph, can you please find some reference material from the web for me to get started?\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING PERSISTANCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You previously asked me to find some reference material to help you get started with learning Langgraph.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"Please Tell me what i said before\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Hi Moosa, it's great to hear from you again! I remember you mentioned that you love Langgraph. Is there anything specific you'd like to discuss or work on today related to Langgraph or any other topic?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"Hi, I am Moosa and i love langgraph\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"111\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I don't have access to that information. Could you please share your name with me?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"What is my Name?\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"26\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, I do! Based on the information I have, you have a Python test scheduled for Friday at 10 AM. The test will cover all major topics. Is there anything else I can help you with regarding your test?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"Do you know about my test\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Okay, I've reviewed the LangChain PDF you uploaded. Here's a summary of the key points:\n",
       "\n",
       "**LangChain Overview:**\n",
       "LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It enhances the interaction between LLMs and various data sources, tools, and APIs, making it easier to build efficient, scalable, and manageable applications.\n",
       "\n",
       "**Key Features:**\n",
       "*   **Chains:** LangChain allows you to create sequences of actions that combine LLM calls with other operations like data retrieval, transformation, or API calls. This enables multi-step reasoning.\n",
       "*   **Document Loading and Processing:** It provides tools to load data from various sources (local files, databases, web sources) and process them for efficient retrieval. This includes indexing documents and converting them into embeddings.\n",
       "*   **Retrieval Augmented Generation (RAG):** LangChain supports RAG, allowing LLMs to retrieve relevant documents based on a query, enhancing the context and relevance of the generated output.\n",
       "*   **Integrations:** It integrates with various LLMs (like OpenAI's GPT models), data sources (documents, APIs, databases), and tools, enabling complex workflows.\n",
       "*   **SQL and Document Querying:** LangChain allows direct querying of SQL databases and indexed documents, providing contextual and relevant retrieval.\n",
       "\n",
       "**LangChain Workflow:**\n",
       "1.  **Document Loading:** Loading documents from external sources.\n",
       "2.  **Document Processing:** Processing and indexing documents for efficient retrieval.\n",
       "3.  **Querying and Retrieval:** Fetching relevant documents based on a user query.\n",
       "4.  **Chain or Agent Execution:** Passing retrieved documents to an LLM for generation, which may involve additional tools.\n",
       "5.  **Final Output:** Generating the final output based on processed information.\n",
       "\n",
       "**Ecosystem:**\n",
       "LangChain has a growing ecosystem that includes various tools and integrations to support different functionalities.\n",
       "\n",
       "**Use Cases:**\n",
       "LangChain is applicable in various domains, including but not limited to:\n",
       "*   Chatbots and conversational AI\n",
       "*   Data analysis and processing\n",
       "*   Content generation\n",
       "*   Question answering systems\n",
       "\n",
       "**Components:**\n",
       "*   **LLMs:** Interfaces with various LLMs like OpenAI, Cohere, and Anthropic.\n",
       "*   **Data Sources:** Supports integrations with documents, APIs, and databases.\n",
       "\n",
       "**Performance:**\n",
       "For large-scale deployments, performance optimization is crucial to ensure scalability, speed, and responsiveness.\n",
       "\n",
       "**Conclusion:**\n",
       "LangChain enhances the capabilities of LLMs by enabling complex workflows, tool integration, and data processing, making it a valuable tool for developers working with LLMs.\n",
       "\n",
       "Is there anything specific you would like to explore further or any other questions you have about the document?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"I have uploaded a langchain pdf, Please Summarize the whole document\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING STUDY SCHEDULE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"12\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I will create a study schedule for you based on your preferences. You have a Python test on Friday morning, you are available to study in the mornings, and you want to study for at most 3 hours each day. You want to cover all major topics in Python and prefer at least half-hour breaks between study sessions. You also want to include all days until the test from now.\n",
       "\n",
       "Since today is December 28th, 2024, I will create a schedule for today, December 28th, 29th, 30th, 31st, and January 1st.\n",
       "\n",
       "To make the schedule effective, I will allocate 3 hours each day with half-hour breaks between sessions. I will also include a mix of focused study and review sessions.\n",
       "\n",
       "Here is the study schedule:\n",
       "\n",
       "**Saturday, December 28th:**\n",
       "*   **9:00 AM - 10:00 AM:** Focused Study - Python Basics (Data types, variables, operators)\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Focused Study - Control Structures (if, for, while loops)\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Review - Python Basics and Control Structures\n",
       "\n",
       "**Sunday, December 29th:**\n",
       "*   **9:00 AM - 10:00 AM:** Focused Study - Functions and Modules\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Focused Study - Working with Files\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Review - Functions, Modules, and File Handling\n",
       "\n",
       "**Monday, December 30th:**\n",
       "*   **9:00 AM - 10:00 AM:** Focused Study - Object-Oriented Programming (Classes, Objects)\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Focused Study - Exception Handling\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Review - OOP and Exception Handling\n",
       "\n",
       "**Tuesday, December 31st:**\n",
       "*   **9:00 AM - 10:00 AM:** Focused Study - Working with Libraries (e.g., NumPy, Pandas)\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Focused Study - Data Structures (Lists, Dictionaries, Tuples)\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Review - Libraries and Data Structures\n",
       "\n",
       "**Wednesday, January 1st:**\n",
       "*   **9:00 AM - 10:00 AM:** Practice - Solving coding problems\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Review - All topics\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Practice - Mock test\n",
       "\n",
       "**Thursday, January 2nd:**\n",
       "*   **9:00 AM - 10:00 AM:** Final Review - Quick review of all topics\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Practice - Mock test\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Relax and prepare for the test\n",
       "\n",
       "I will now save this schedule to a markdown file.\n",
       "\n",
       "Okay, I've created a study schedule for you, focusing on major Python topics and including breaks. The schedule is saved in `study_schedule.md`. Let me know if you need any adjustments!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\n",
    "        \"query\": \"Hi, I Like python very much and have a python test upcoming on friday, please create a study schedule for me\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I have already saved the schedule in a markdown file named `study_schedule.md`. Is there anything else I can help you with?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"Thats Great! Please save the schedule in a markdown file\"},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
