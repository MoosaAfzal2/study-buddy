{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Study Buddy AI: Your Personalized Study Assistant**\n",
    "\n",
    "<!-- ##### **Description:**\n",
    "Study Buddy AI is an intelligent assistant designed to revolutionize the way students learn and plan their studies. Equipped with advanced features like personalized study plans, web search for research, math problem-solving, and human-in-the-loop assistance, it ensures accurate, actionable, and tailored support for academic success. Leveraging memory and contextual knowledge, Study Buddy AI creates dynamic, efficient schedules and provides step-by-step explanations to help students master any subject with ease. -->\n",
    "\n",
    "##### **Current Features:**\n",
    "\n",
    "- **Tool Assistance**: Integrates with tools like Study Scheduler and Web Search.\n",
    "- **Personalized Support**: Remembers user-specific details such as exam dates, preferred study times, and past topics to deliver customized responses.\n",
    "- **Study Schedule Creation**: Automatically generates optimized study plans based on deadlines, topics, and available time.\n",
    "- **User-Friendly Interaction**: Polite, clear, and concise responses with follow-up questions to ensure clarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEPENDENCIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U google-generativeai langchain-google-genai langgraph langchain langchain-core pydantic tavily-python langchain_community langgraph-checkpoint-sqlite chromadb langchain-text-splitters pypdf rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"GOOGLE_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "# _set_env(\"LANGCHAIN_API_KEY\")\n",
    "# _set_env(\"LANGCHAIN_PROJECT\")\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Annotated, Any, Dict, Union, Literal, Type\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import uuid\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END, add_messages\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition, InjectedState, InjectedStore\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain_core.tools import tool, InjectedToolArg\n",
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    AnyMessage,\n",
    "    RemoveMessage,\n",
    "    ToolMessage,\n",
    "    AIMessageChunk,\n",
    ")\n",
    "\n",
    "from IPython.display import Image, clear_output, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would generally get the user_id from the authenticated User\n",
    "user_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_chat_template(\n",
    "    model: BaseChatModel,\n",
    "    template: str,\n",
    "    template_data: dict[str, Any],\n",
    ") -> BaseMessage:\n",
    "    \"\"\"Invoke the chat template with the provided tenplate data and return the response.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model\n",
    "    result = chain.invoke(template_data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def stream_agent(\n",
    "    agent: CompiledStateGraph,\n",
    "    input_value: Dict | List[Dict],\n",
    "    config: RunnableConfig,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    A function to interact with an agent in a streaming manner, handling updates and messages\n",
    "    during the stream. The function facilitates communication with the agent by providing\n",
    "    answers to questions when prompted and displaying messages based on specific conditions.\n",
    "\n",
    "    Args:\n",
    "        agent (CompiledStateGraph):\n",
    "            The compiled state graph representing the agent's behavior or knowledge graph.\n",
    "            This is used to drive the state transitions and handle the stream processing.\n",
    "\n",
    "        input_value (Dict | List[Dict]):\n",
    "            The input data that is passed to the agent. This can either be a single dictionary or\n",
    "            a list of dictionaries. These inputs are processed by the agent to generate stream outputs.\n",
    "\n",
    "        config (RunnableConfig):\n",
    "            The configuration object that governs how the agent runs. This may include settings\n",
    "            related to execution parameters, logging, error handling, and any other agent-specific configurations.\n",
    "\n",
    "        debug (bool):\n",
    "            If True, enables detailed logging for debugging purposes.\n",
    "\n",
    "    Returns:\n",
    "        None: This function is designed to interact with the agent and display messages interactively.\n",
    "              It doesn't return a value but may perform side effects such as displaying output.\n",
    "    \"\"\"\n",
    "    message = \"\"\n",
    "    for stream_mode, data in agent.stream(\n",
    "        input=input_value,\n",
    "        config=config,\n",
    "        stream_mode=[\"updates\", \"messages\"],\n",
    "        debug=debug,\n",
    "    ):\n",
    "        interrupt = (\n",
    "            data.get(\"__interrupt__\", None)\n",
    "            if isinstance(data, dict) and stream_mode == \"updates\"\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        if stream_mode == \"updates\" and interrupt and len(interrupt) > 0:\n",
    "            questions = interrupt[0].value\n",
    "            questions_with_ans = []\n",
    "            for question in questions:\n",
    "                user_input = input(question.get(\"question\", \"\"))\n",
    "\n",
    "                questions_with_ans.append({**question, \"answer\": user_input})\n",
    "\n",
    "            stream_agent(\n",
    "                agent=agent,\n",
    "                config=config,\n",
    "                input_value=Command(resume=questions_with_ans),\n",
    "            )\n",
    "\n",
    "        elif (\n",
    "            stream_mode == \"messages\"\n",
    "            and data[-1].get(\"langgraph_node\") == \"handle_user_query\"\n",
    "            and data[0].content\n",
    "            and isinstance(data[0], (AIMessage, AIMessageChunk))\n",
    "        ):\n",
    "            message += data[0].content  # Append content without a newline\n",
    "            clear_output(True)\n",
    "            display(Markdown(message))  # Display the message as Markdown\n",
    "\n",
    "\n",
    "def load_and_split_documents(\n",
    "    docs_folder: str,\n",
    "    file_extension: str,\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 50,\n",
    "    loader_cls=None,\n",
    "    loader_kwargs=None,\n",
    "    text_splitter_language=Language.MARKDOWN,\n",
    "    disable_splitter: bool = False,\n",
    "    metadata: dict = None,\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Load and split documents from a directory, based on file extension.\n",
    "\n",
    "    Args:\n",
    "    - docs_folder (str): Path to the folder containing documents.\n",
    "    - file_extension (str): The file extension to match (e.g., '.txt', '.pdf').\n",
    "    - chunk_size (int): The size of each chunk for text splitting.\n",
    "    - chunk_overlap (int): The overlap size for text splitting.\n",
    "    - loader_cls (class): The class to use for loading files (TextLoader or PyPDFLoader).\n",
    "    - loader_kwargs (dict): Additional arguments for the loader class.\n",
    "    - text_splitter_language (Language): The language setting for text splitting.\n",
    "    - disable_splitter (bool): For Disabling Splitter. defaults to False\n",
    "    - metadata (dict): Additional metadata for docs. defaults to None\n",
    "\n",
    "    Returns:\n",
    "    - List of split document chunks.\n",
    "    \"\"\"\n",
    "    if loader_kwargs is None:\n",
    "        loader_kwargs = {}\n",
    "\n",
    "    # Define a text splitter with the chosen language\n",
    "    text_splitter = (\n",
    "        RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "        if disable_splitter\n",
    "        else RecursiveCharacterTextSplitter.from_language(\n",
    "            language=text_splitter_language,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Initialize directory loader\n",
    "    file_loader = DirectoryLoader(\n",
    "        path=docs_folder,\n",
    "        glob=f\"**/*{file_extension}\",\n",
    "        show_progress=True,\n",
    "        silent_errors=True,\n",
    "        loader_cls=loader_cls,\n",
    "        loader_kwargs=loader_kwargs,\n",
    "    )\n",
    "\n",
    "    # Load and split documents\n",
    "    loaded_docs = file_loader.load_and_split(text_splitter)\n",
    "\n",
    "    docs = (\n",
    "        [\n",
    "            Document(\n",
    "                **doc.model_dump(exclude=\"metadata\"),\n",
    "                metadata={**doc.metadata, **metadata},\n",
    "            )\n",
    "            for doc in loaded_docs\n",
    "        ]\n",
    "        if metadata\n",
    "        else loaded_docs\n",
    "    )\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"gemini-1.5-flash\"\n",
    "MODEL_NAME = \"gemini-2.0-flash-exp\"\n",
    "EMBEDDING_MODEL_NAME = \"models/embedding-001\"\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0)\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a TypedDict for each file type configuration\n",
    "class FileConfig(TypedDict):\n",
    "    type: str  # File extension, e.g., \".txt\", \".pdf\"\n",
    "    loader: Type  # The loader class, e.g., TextLoader, PyPDFLoader\n",
    "    text_splitter_language: Language  # The language for the text splitter\n",
    "    loader_kwargs: dict  # Additional keyword arguments for the loader class\n",
    "    disable_splitter: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the file types and their respective loaders and text splitter languages\n",
    "supported_files_configs: List[FileConfig] = [\n",
    "    {\n",
    "        \"type\": \".txt\",\n",
    "        \"loader\": TextLoader,\n",
    "        \"text_splitter_language\": Language.MARKDOWN,\n",
    "        \"loader_kwargs\": {\"autodetect_encoding\": True},\n",
    "        \"metadata\": {\"user_id\": str(user_id)},\n",
    "    },\n",
    "    {\n",
    "        \"type\": \".md\",\n",
    "        \"loader\": TextLoader,\n",
    "        \"text_splitter_language\": Language.MARKDOWN,\n",
    "        \"loader_kwargs\": {\"autodetect_encoding\": True},\n",
    "        \"metadata\": {\"user_id\": str(user_id)},\n",
    "    },\n",
    "    {\n",
    "        \"type\": \".pdf\",\n",
    "        \"loader\": PyPDFLoader,\n",
    "        \"text_splitter_language\": Language.MARKDOWN,\n",
    "        \"loader_kwargs\": {\"extract_images\": True},\n",
    "        \"metadata\": {\"user_id\": str(user_id)},\n",
    "    },\n",
    "    {\n",
    "        \"type\": \".csv\",\n",
    "        \"loader\": CSVLoader,\n",
    "        \"disable_splitter\": True,\n",
    "        \"metadata\": {\"user_id\": str(user_id)},\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 52.93it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Example of using the function for various file types (text and PDF):\n",
    "docs_folder = \"./docs\"\n",
    "docs = []\n",
    "\n",
    "# Loop through each file configuration and load the corresponding documents\n",
    "for config in supported_files_configs:\n",
    "    file_extension = config[\"type\"]\n",
    "    loader_cls = config[\"loader\"]\n",
    "    text_splitter_language = config.get(\"text_splitter_language\")\n",
    "    loader_kwargs = config.get(\"loader_kwargs\", {})\n",
    "    metadata = config.get(\"metadata\", None)\n",
    "    disable_splitter = config.get(\"disable_splitter\", None)\n",
    "\n",
    "    # Load and split documents for the current file type\n",
    "    docs += load_and_split_documents(\n",
    "        docs_folder=docs_folder,\n",
    "        file_extension=file_extension,\n",
    "        loader_cls=loader_cls,\n",
    "        loader_kwargs=loader_kwargs,\n",
    "        text_splitter_language=text_splitter_language,\n",
    "        disable_splitter=disable_splitter,\n",
    "        metadata=metadata,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector DB (Chroma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "vector_db_path = \"./vector_db\"\n",
    "# Create the directory if it doesn't exist\n",
    "Path(vector_db_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"vector-chroma-db\",\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=vector_db_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputState(TypedDict):\n",
    "    query: str\n",
    "\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    response: AIMessage\n",
    "\n",
    "\n",
    "class OverallState(InputState, OutputState):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    summary: Optional[str] = None\n",
    "    memories: Optional[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERSISTANCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In memory\n",
    "conn = sqlite3.connect(\":memory:\", check_same_thread=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file db already exists.\n"
     ]
    }
   ],
   "source": [
    "# # pull file if it doesn't exist and connect to local db\n",
    "!mkdir db && [ ! -f db/sqlite.db ] && wget -P db https://github.com/MoosaAfzal2/study-buddy/db/sqlite.db && wget -P db https://github.com/MoosaAfzal2/study-buddy/db/sqlite.db-wal\n",
    "\n",
    "db_path = \"db/sqlite.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the Checkpointer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our checkpointer\n",
    "checkpointer: SqliteSaver = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMORY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Memory Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embedding_model,\n",
    "        \"dims\": 1536,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMESPACE_NAME = \"memories\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ProgressType(TypedDict):\n",
    "#     \"\"\"\n",
    "#     Defines the structure of progress for a study topic as a percentage.\n",
    "#     Each progress entry includes the topic name and its completion percentage.\n",
    "#     \"\"\"\n",
    "\n",
    "#     topic: Optional[str] = None  # The name of the study topic (e.g., \"Algebra\")\n",
    "#     progress_percentage: Optional[float] = (\n",
    "#         None  # The completion percentage for the topic (e.g., 75.0)\n",
    "#     )\n",
    "\n",
    "\n",
    "class MemoryType(BaseModel):\n",
    "    \"\"\"General memory structure for a Study Buddy Agent to store memory\"\"\"\n",
    "\n",
    "    content: str = Field(\n",
    "        description='The main content of the memory. For example: \"User expressed interest in learning about French.\"'\n",
    "    )\n",
    "\n",
    "    context: str = Field(\n",
    "        description='Additional context for the memory. For example: \"This was mentioned while discussing career options in Europe.\"'\n",
    "    )\n",
    "\n",
    "    # # User Information (Tracking user-specific details)\n",
    "    # username: Optional[str] = Field(\n",
    "    #     default=None, description='The username of the user (e.g., \"john_doe\")'\n",
    "    # )\n",
    "\n",
    "    # age: Optional[int] = Field(\n",
    "    #     default=None, description=\"The age of the user (e.g., 21)\"\n",
    "    # )\n",
    "\n",
    "    # gender: Optional[str] = Field(\n",
    "    #     default=None,\n",
    "    #     description='The gender of the user (e.g., \"Male\", \"Female\", \"Non-binary\")',\n",
    "    # )\n",
    "\n",
    "    # preferences: Optional[str] = Field(\n",
    "    #     default=None,\n",
    "    #     description=\"User preferences, such as preferred study time, study methods, etc.\",\n",
    "    # )\n",
    "\n",
    "    # # Study Preferences\n",
    "    # study_duration: Optional[int] = Field(\n",
    "    #     default=None,\n",
    "    #     description=\"Preferred study duration in minutes (e.g., 60 minutes)\",\n",
    "    # )\n",
    "\n",
    "    # break_duration: Optional[int] = Field(\n",
    "    #     default=None,\n",
    "    #     description=\"Preferred break duration in minutes (e.g., 15 minutes)\",\n",
    "    # )\n",
    "\n",
    "    # # Scheduling and Deadlines\n",
    "    # reminders: List[ReminderType] = Field(\n",
    "    #     default=[],\n",
    "    #     description='List of reminders with descriptions and dates (e.g., [{\"description\": \"Study for Math exam\", \"date\": \"2024-12-15\"}])',\n",
    "    # )\n",
    "\n",
    "    # deadlines: List[ReminderType] = Field(\n",
    "    #     default=[],\n",
    "    #     description='List of deadlines with event names and corresponding dates (e.g., [{\"description\": \"Assignment deadline\", \"date\": \"2024-12-18\"}])',\n",
    "    # )\n",
    "\n",
    "    # # Study Topics\n",
    "    # topics_to_study: List[str] = Field(\n",
    "    #     default=[], description='List of topics to study (e.g., [\"Algebra\", \"History\"])'\n",
    "    # )\n",
    "\n",
    "    # completed_topics: List[str] = Field(\n",
    "    #     default=[],\n",
    "    #     description='List of topics that have been completed (e.g., [\"Calculus\", \"Literature\"])',\n",
    "    # )\n",
    "\n",
    "    # # Progress Tracking\n",
    "    # progress: List[ProgressType] = Field(\n",
    "    #     default=[],\n",
    "    #     description='Progress for each study topic as a percentage (e.g., [{\"topic\": \"Algebra\", \"progress_percentage\": 75.0}])',\n",
    "    # )\n",
    "\n",
    "    # # Personal Notes and Goals\n",
    "    # personal_notes: Optional[str] = Field(\n",
    "    #     default=None,\n",
    "    #     description=\"Space for personal goals, motivational notes, or other reminders\",\n",
    "    # )\n",
    "\n",
    "    # # Last Updated Timestamp\n",
    "    # last_updated: Optional[str] = Field(\n",
    "    #     default=None,\n",
    "    #     description='Timestamp for when the memory was last updated (e.g., \"2024-12-15T14:30:00\")',\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOOLS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Search Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_search_tool = TavilySearchResults(\n",
    "    name=\"tavily_web_search\",\n",
    "    max_results=3,\n",
    "    # include_answer=True,\n",
    "    # include_raw_content=True,\n",
    "    # include_images=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask Human Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Questions(BaseModel):\n",
    "    question: str\n",
    "    answer: Optional[str] = None\n",
    "\n",
    "\n",
    "@tool\n",
    "def ask_human(questions: List[Questions]):\n",
    "    \"\"\"\n",
    "    Prompts the user with a list of questions and captures their answers.\n",
    "\n",
    "    This function is designed to ask a series of questions to a human and store\n",
    "    their responses. Each question is represented by a dictionary that contains\n",
    "    an optional `id`, the `question` text, and an optional `answer`.\n",
    "\n",
    "    Args:\n",
    "        questions (List[Questions]): A list of question dictionaries. Each dictionary must contain the following keys:\n",
    "            - `question` (str): The question text to be presented to the user.\n",
    "            - `answer` (Optional[str], optional): The user's answer to the question. This is optional and may be left blank if the answer is unknown or pending. A default answer value may also be provided if desired.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of dictionaries representing the user's responses. Each dictionary will contain the `id` and `answer` for each question. The `answer` field will hold the user-provided response or remain empty if not provided.\n",
    "    \"\"\"\n",
    "\n",
    "    user_input = interrupt(\n",
    "        value=[\n",
    "            {\"id\": str(uuid.uuid4()), **question.model_dump()} for question in questions\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return user_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study Scheduling Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudySession(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a study session with details about the subject, time, notes.\n",
    "    \"\"\"\n",
    "\n",
    "    day: str = Field(\n",
    "        description='The day of the week for the study session (e.g., \"Monday\")'\n",
    "    )\n",
    "    subject: str = Field(description='The subject being studied (e.g., \"Math\")')\n",
    "    start_time: str = Field(\n",
    "        description='Start time of the session in \"HH:MM\" format (e.g., \"09:00\")'\n",
    "    )\n",
    "    end_time: str = Field(\n",
    "        description='End time of the session in \"HH:MM\" format (e.g., \"11:00\")'\n",
    "    )\n",
    "    notes: str = Field(\n",
    "        description='Additional notes for the session (e.g., \"Focus on Algebra\")'\n",
    "    )\n",
    "    priority: str = Field(\n",
    "        description='Priority of the study session (e.g., \"High\", \"Medium\", \"Low\")'\n",
    "    )\n",
    "    estimated_completion_time: int = Field(\n",
    "        description=\"Estimated time for completion of the subject in minutes\"\n",
    "    )\n",
    "    study_type: str = Field(\n",
    "        description='Type of study session (e.g., \"Focused\", \"Review\")'\n",
    "    )\n",
    "    status: str = Field(description='Completion status (e.g., \"Completed\", \"Pending\")')\n",
    "    study_notes: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description='Specific study notes for the session (e.g., \"Focus on chapters 3-5\")',\n",
    "    )\n",
    "    available_time_per_day: int = Field(\n",
    "        description=\"Total study time available per day in minutes\"\n",
    "    )\n",
    "\n",
    "\n",
    "@tool\n",
    "def create_study_schedule(\n",
    "    study_sessions: List[StudySession],  # List of StudySession dictionaries\n",
    ") -> List[StudySession]:\n",
    "    \"\"\"\n",
    "    Creates a personalized study schedule based on user preferences, subjects, and available time.\n",
    "\n",
    "    This function takes a list of study sessions, calculates the required time for each, and creates a study\n",
    "    schedule based on the available time per day. It also ensures that the number of sessions does not exceed\n",
    "    the available time, provides a break between consecutive study sessions, and includes warnings when the\n",
    "    available time is insufficient for all sessions.\n",
    "\n",
    "    Args:\n",
    "        study_sessions (List[StudySession]): A list of dictionaries, each representing a study session. Each dictionary must contain the following keys:\n",
    "            - 'day': The day of the week for the study session (e.g., \"Monday\").\n",
    "            - 'subject': The subject being studied (e.g., \"Math\").\n",
    "            - 'start_time': The start time of the session in \"HH:MM\" format.\n",
    "            - 'end_time': The end time of the session in \"HH:MM\" format.\n",
    "            - 'notes': Additional notes for the session (e.g., \"Focus on Algebra\").\n",
    "            - 'priority': The priority of the session (e.g., \"High\", \"Medium\", \"Low\").\n",
    "            - 'estimated_completion_time': The estimated duration for completing the session in minutes.\n",
    "            - 'study_type': The type of session (e.g., \"Focused\", \"Review\").\n",
    "            - 'status': The completion status of the session (e.g., \"Completed\", \"Pending\").\n",
    "            - 'study_notes': Specific study notes for the session (optional).\n",
    "            - 'available_time_per_day': The total study time available per day in minutes.\n",
    "\n",
    "    Returns:\n",
    "        List[StudySession]: A list of dictionaries representing the generated study schedule. Each dictionary contains the following keys:\n",
    "            - 'day': The day of the week for the study session.\n",
    "            - 'subject': The subject being studied.\n",
    "            - 'start_time': The start time of the session in \"HH:MM\" format.\n",
    "            - 'end_time': The end time of the session in \"HH:MM\" format.\n",
    "            - 'notes': Additional notes for the session.\n",
    "            - 'priority': The priority of the session.\n",
    "            - 'estimated_completion_time': The estimated duration for completing the session.\n",
    "            - 'study_type': The type of session.\n",
    "            - 'status': The completion status of the session.\n",
    "            - 'study_notes': Specific study notes for the session.\n",
    "            - 'available_time_per_day': The total study time available per day.\n",
    "            - Breaks: A break is included after each session unless it’s the last session.\n",
    "\n",
    "    Warnings:\n",
    "        - If the total time for all sessions exceeds the available time per day, the function will try to fit as many sessions as possible.\n",
    "        - If there is remaining time after scheduling, a message will be shown to inform the user.\n",
    "    \"\"\"\n",
    "\n",
    "    schedule: List = []\n",
    "    current_day = datetime.now()\n",
    "\n",
    "    for session in study_sessions:\n",
    "        # Parse start_time into a datetime object\n",
    "        start_time = datetime.strptime(session.start_time, \"%H:%M\").replace(\n",
    "            year=current_day.year, month=current_day.month, day=current_day.day\n",
    "        )\n",
    "\n",
    "        # Adjust start time if it has already passed today\n",
    "        if start_time < current_day:\n",
    "            start_time += timedelta(days=1)  # Schedule for the next day\n",
    "\n",
    "        # Calculate end_time based on estimated duration\n",
    "        end_time = start_time + timedelta(minutes=session.estimated_completion_time)\n",
    "\n",
    "        # Create and add the study session to the schedule\n",
    "        new_session = StudySession(\n",
    "            day=start_time.strftime(\"%A\"),\n",
    "            subject=session.subject,\n",
    "            start_time=start_time.strftime(\"%H:%M\"),\n",
    "            end_time=end_time.strftime(\"%H:%M\"),\n",
    "            notes=session.notes,\n",
    "            priority=session.priority,\n",
    "            estimated_completion_time=session.estimated_completion_time,\n",
    "            study_type=session.study_type,\n",
    "            status=session.status,\n",
    "            study_notes=session.study_notes or \"\",  # Use empty string if None\n",
    "            available_time_per_day=session.available_time_per_day,\n",
    "        )\n",
    "\n",
    "        schedule.append(new_session.model_dump())\n",
    "\n",
    "        # Add a break time after each session (unless it's the last session)\n",
    "        if session != study_sessions[-1]:\n",
    "            break_time_start = end_time\n",
    "            break_time_end = break_time_start + timedelta(minutes=15)\n",
    "            break_session = StudySession(\n",
    "                day=start_time.strftime(\"%A\"),\n",
    "                subject=\"Break\",\n",
    "                start_time=break_time_start.strftime(\"%H:%M\"),\n",
    "                end_time=break_time_end.strftime(\"%H:%M\"),\n",
    "                notes=\"Take a 15-minute break\",\n",
    "                priority=\"Low\",\n",
    "                estimated_completion_time=15,\n",
    "                study_type=\"Break\",\n",
    "                status=\"Pending\",\n",
    "                study_notes=\"\",\n",
    "                available_time_per_day=session.available_time_per_day,\n",
    "            )\n",
    "\n",
    "            schedule.append(break_session.model_dump())\n",
    "\n",
    "    return schedule\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_study_schedule_to_markdown(\n",
    "    markdown: str,\n",
    "    file_name: str = \"study_schedule.md\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Saves the study schedule content into a Markdown file, ensuring proper line breaks and formatting.\n",
    "\n",
    "    Args:\n",
    "        markdown (str): The content in Markdown format to be saved.\n",
    "        file_name (str): The name of the Markdown file where the schedule will be saved (default is 'study_schedule.md').\n",
    "\n",
    "    Returns:\n",
    "        str: Success Messages\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure line breaks are properly handled and formatting is correct\n",
    "        formatted_markdown = markdown.replace(\"\\\\n\", \"\\n\")  # Ensure proper line breaks\n",
    "\n",
    "        # Open the file in write mode and save the Markdown content\n",
    "        with open(\"./docs/\" + file_name, mode=\"w\") as file:\n",
    "            file.write(formatted_markdown)\n",
    "        return f\"Study schedule saved successfully to {file_name}\"\n",
    "\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied when trying to write to {file_name}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Directory or file path {file_name} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving study schedule: {e}\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_study_schedule_to_csv(\n",
    "    schedule_data: List[StudySession],\n",
    "    file_name: str = \"study_schedule.csv\",\n",
    "    field_names: List[str] = [\n",
    "        \"day\",\n",
    "        \"subject\",\n",
    "        \"start_time\",\n",
    "        \"end_time\",\n",
    "        \"notes\",\n",
    "        \"priority\",\n",
    "        \"estimated_completion_time\",\n",
    "        \"study_type\",\n",
    "        \"status\",\n",
    "        \"study_notes\",\n",
    "        \"available_time_per_day\",\n",
    "    ],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Saves the study schedule to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        schedule_data (List[StudySession]): A list of dictionaries, each representing a study session. Each dictionary must contain the following keys:\n",
    "            - 'day': The day of the week for the study session (e.g., \"Monday\").\n",
    "            - 'subject': The subject being studied (e.g., \"Math\").\n",
    "            - 'start_time': The start time of the session in \"HH:MM\" format.\n",
    "            - 'end_time': The end time of the session in \"HH:MM\" format.\n",
    "            - 'notes': Additional notes for the session (e.g., \"Focus on Algebra\").\n",
    "            - 'priority': The priority of the session (e.g., \"High\", \"Medium\", \"Low\").\n",
    "            - 'estimated_completion_time': The estimated duration for completing the session in minutes.\n",
    "            - 'study_type': The type of session (e.g., \"Focused\", \"Review\").\n",
    "            - 'status': The completion status of the session (e.g., \"Completed\", \"Pending\").\n",
    "            - 'study_notes': Specific study notes for the session (optional).\n",
    "            - 'available_time_per_day': The total study time available per day in minutes.\n",
    "        file_name (str): The name of the CSV file where the schedule will be saved (default is 'study_schedule.csv').\n",
    "        field_names (List[str]): The column names for the CSV file (default is [\"day\",\"subject\",\"start_time\",\"end_time\",\"notes\",\"priority\",\"estimated_completion_time\",\"study_type\",\"status\",\"study_notes\",\"available_time_per_day\"]).\n",
    "\n",
    "    Returns:\n",
    "        str: Success Messages\n",
    "    \"\"\"\n",
    "\n",
    "    # Dynamically fetch required keys from StudySession TypedDict\n",
    "    required_keys = set(StudySession.__annotations__.keys())  # Set of required keys\n",
    "\n",
    "    for session in schedule_data:\n",
    "        if not required_keys.issubset(session.model_dump().keys()):\n",
    "            raise ValueError(\n",
    "                f\"Each schedule entry must contain the following keys: {', '.join(required_keys)}\"\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        with open(\"./docs/\" + file_name, mode=\"w\", newline=\"\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=field_names)\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Handling falsy or empty values for any field in the session\n",
    "            for session in schedule_data:\n",
    "                session_dict = (\n",
    "                    session.model_copy().model_dump()\n",
    "                )  # Create a copy of the session to avoid modifying the original\n",
    "\n",
    "                # Iterate through each field and handle falsy or empty values\n",
    "                for key, value in session_dict.items():\n",
    "                    if value in (None, \"\", [], {}, 0, False):  # Check for falsy values\n",
    "                        if key == \"study_notes\":\n",
    "                            session_dict[key] = (\n",
    "                                \"\"  # Empty study_notes should be an empty string\n",
    "                            )\n",
    "                        else:\n",
    "                            session_dict[key] = (\n",
    "                                \"N/A\"  # Default for other fields (e.g., \"N/A\" or 0)\n",
    "                            )\n",
    "\n",
    "                # Write the session row\n",
    "                writer.writerow(session_dict)\n",
    "\n",
    "        return f\"Study schedule saved successfully to {file_name}\"\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied when trying to write to {file_name}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_name} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving study schedule: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def upsert_memory(\n",
    "    memory: MemoryType,\n",
    "    *,\n",
    "    memory_id: Optional[Union[str, uuid.UUID]] = None,\n",
    "    # Hide these arguments from the model.\n",
    "    config: Annotated[RunnableConfig, InjectedToolArg],\n",
    "    store: Annotated[BaseStore, InjectedStore],\n",
    "):\n",
    "    \"\"\"Upsert a memory in the database.\n",
    "\n",
    "    If a memory conflicts with an existing one, then just UPDATE the\n",
    "    existing one by passing in memory_id - don't create two memories\n",
    "    that are the same. If the user corrects a memory, UPDATE it.\n",
    "\n",
    "    Args:\n",
    "        memory (MemoryType): A memory dictionary containing the following keys:\n",
    "            - `content`: The main content of the memory. For example: \"User expressed interest in learning about French.\"\n",
    "            - `context`: Additional context for the memory. For example: \"This was mentioned while discussing career options in Europe.\"\n",
    "        memory_id: ONLY PROVIDE IF UPDATING AN EXISTING MEMORY. The memory to overwrite.\n",
    "    \"\"\"\n",
    "    mem_id = memory_id or uuid.uuid4()\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    store.put((NAMESPACE_NAME, user_id), key=str(mem_id), value=memory.model_dump())\n",
    "    return f\"Stored memory {mem_id}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def delete_memory(\n",
    "    memory_id: uuid.UUID | str,\n",
    "    config: RunnableConfig,\n",
    "    store: Annotated[BaseStore, InjectedToolArg],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Delete a memory from the database by its ID.\n",
    "    This function removes a specific memory from the store based on the provided\n",
    "    memory ID. The memory ID must exist in the store for the deletion to be successful.\n",
    "\n",
    "    Args:\n",
    "        memory_id (uuid.UUID | str): The ID of the memory to delete.\n",
    "\n",
    "    Returns:\n",
    "        str: A message confirming whether the memory was successfully deleted or if the memory ID was not found.\n",
    "    \"\"\"\n",
    "\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    memory_key = str(memory_id)\n",
    "\n",
    "    # Check if the memory exists before deleting\n",
    "    memory = store.get((NAMESPACE_NAME, user_id), key=memory_key)\n",
    "\n",
    "    if memory:\n",
    "        store.delete((NAMESPACE_NAME, user_id), key=memory_key)\n",
    "        return f\"Memory {memory_id} successfully deleted.\"\n",
    "    else:\n",
    "        return f\"Memory {memory_id} not found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SearchKwargs(TypedDict):\n",
    "#     search_type: Optional[str] = \"similarity\"  # Default value \"similarity\"\n",
    "#     k: Optional[int] = 2  # Number of documents to return\n",
    "#     score_threshold: Optional[float] = None  # Minimum similarity score threshold\n",
    "#     fetch_k: Optional[int] = 20  # For MMR algorithm\n",
    "#     lambda_mult: Optional[float] = 0.5  # For MMR algorithm, 0-1\n",
    "#     filter: Optional[Dict] = Field(None)  # Additional filters for documents\n",
    "        \n",
    "@tool\n",
    "def retrieve_documents(\n",
    "    query: str,\n",
    "    # kwargs: SearchKwargs,\n",
    "    *,\n",
    "    config: Annotated[RunnableConfig, InjectedToolArg],\n",
    "):\n",
    "    \"\"\"\n",
    "    Search and return user documents\n",
    "\n",
    "    Args:\n",
    "        query (str): A memory dictionary containing the following keys:\n",
    "    \"\"\"\n",
    "        # kwargs (SearchKwargs): dictionary of arguments to pass to the search function with keys:\n",
    "        #     - `search_type` (Optional[str]): Defines the type of search that the Retriever should perform. Can be \"similarity\" (default), \"mmr\", or \"similarity_score_threshold\".\n",
    "        #     - `k`: Amount of documents to return (Default: 4)\n",
    "        #     - `score_threshold`: Minimum relevance threshold for similarity_score_threshold\n",
    "        #     - `fetch_k`: Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "        #     - `lambda_mult`: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "        #     - `filter`: Filter by document metadata\n",
    "    # \"\"\"  \n",
    "    # # Ensure kwargs is initialized to an empty SearchKwargs model if None is passed\n",
    "    # kwargs = kwargs or SearchKwargs()\n",
    "\n",
    "    # # Ensure the user_id is fetched correctly\n",
    "    user_id = str(config.get(\"configurable\", {}).get(\"user_id\", \"\"))\n",
    "\n",
    "    # # Prepare search parameters\n",
    "    # search_kwargs = kwargs  # Avoid modifying original dictionary\n",
    "    # search_kwargs.update({\n",
    "    #     \"filter\": {\"user_id\": user_id},\n",
    "    #     \"k\": kwargs.k,\n",
    "    #     \"score_threshold\": kwargs.score_threshold,\n",
    "    #     \"fetch_k\": kwargs.fetch_k,\n",
    "    #     \"lambda_mult\": kwargs.lambda_mult,\n",
    "    # })\n",
    "\n",
    "    # Initialize the retriever with search parameters\n",
    "    retriever = vector_db.as_retriever(\n",
    "        # search_type=kwargs.search_type,\n",
    "        # search_kwargs=search_kwargs\n",
    "        search_kwargs = {\n",
    "            \"filter\": {\"user_id\": user_id},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    retriever_tool = create_retriever_tool(\n",
    "        retriever, \"retrieve_documents\", \"Search and return user documents\"\n",
    "    )\n",
    "\n",
    "    retrieved_documents = retriever_tool.invoke(input=query, config=config)\n",
    "\n",
    "    return retrieved_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Available Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    tavily_search_tool,\n",
    "    ask_human,\n",
    "    create_study_schedule,\n",
    "    save_study_schedule_to_csv,\n",
    "    save_study_schedule_to_markdown,\n",
    "    upsert_memory,\n",
    "    delete_memory,\n",
    "    retrieve_documents,\n",
    "]\n",
    "\n",
    "model_with_tools = model.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NODES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Node\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_node(\n",
    "    state: OverallState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore,\n",
    ") -> OverallState:\n",
    "    \"\"\"Retrieve memories associated with a user based on a query.\"\"\"\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "\n",
    "    memories = store.search(\n",
    "        (NAMESPACE_NAME, user_id),\n",
    "        query=str([m.content for m in state[\"messages\"][-3:]]),\n",
    "        limit=10,\n",
    "    )\n",
    "\n",
    "    # Format memories for inclusion in the prompt\n",
    "    formatted_memories = \"\\n\".join(\n",
    "        f\"[{mem.key}]: {mem.value} (similarity: {mem.score})\" for mem in memories\n",
    "    )\n",
    "\n",
    "    if formatted_memories:\n",
    "        formatted_memories = f\"\"\"\n",
    "<memories>\n",
    "{formatted_memories}\n",
    "</memories>\n",
    "\"\"\"\n",
    "\n",
    "    return {\"memories\": formatted_memories}\n",
    "\n",
    "\n",
    "class ParsedMemory(BaseModel):\n",
    "    memory_id: Optional[str] = None\n",
    "    memory: MemoryType\n",
    "\n",
    "\n",
    "def update_memory_node(\n",
    "    state: OverallState,\n",
    "    config: RunnableConfig,\n",
    "    store: BaseStore,\n",
    ") -> OverallState:\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    memories = state.get(\"memories\", \"\")\n",
    "    user_query = state.get(\"query\", \"\")\n",
    "    history = summary if summary else state.get(\"messages\", [])\n",
    "\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    namespace = (NAMESPACE_NAME, user_id)\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=ParsedMemory)\n",
    "\n",
    "    # Create our memories prompt\n",
    "    memories_message = ChatPromptTemplate.from_template(\n",
    "        \"This is the current user's memory namespace: {namespace}\\n\\n\"\n",
    "        \"This is summary or history of the conversation to date: {history}\\n\\n\"\n",
    "        \"This is the latest user query: {user_query}\\n\\n\"\n",
    "        \"These are the memories of the user based on the latest query: {memories}\\n\\n\"\n",
    "        \"If a memory conflicts with an existing one, then just UPDATE the existing one by passing in memory_id - don't create two memories that are the same.\"\n",
    "        \"Now based on the above if you think that the current memories should be updated or a new memory should be added then return the new or updated memory in the following Format:\"\n",
    "        \"\"\"\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "        \"EXAMPLES:\"\n",
    "        \" - Creating a New Memory:\"\n",
    "        \"\"\"\n",
    "        {{\n",
    "          \"memory\": {{\n",
    "            \"content\": \"User expressed interest in learning about French.\",\n",
    "            \"context\": \"This was mentioned while discussing career options in Europe.\",\n",
    "            ...other_fields\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \" - Updating a Memory:\"\n",
    "        \"\"\"\n",
    "        {{\n",
    "          \"memory_id\": \"...\",\n",
    "          \"memory\": {{\n",
    "            \"content\": \"User expressed interest in learning about French.\",\n",
    "            \"context\": \"This was mentioned while discussing career options in Europe.\",\n",
    "            ...other_fields\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "    ).format(\n",
    "        format_instructions=parser.get_format_instructions(),\n",
    "        history=history,\n",
    "        memories=memories,\n",
    "        user_query=user_query,\n",
    "        namespace=str(namespace),\n",
    "    )\n",
    "\n",
    "    response = model.invoke(memories_message)\n",
    "\n",
    "    parsed_response = parser.parse(response.content)\n",
    "\n",
    "    mem_id = parsed_response.memory_id or uuid.uuid4()\n",
    "    mem_type = parsed_response.memory_type or \"general\"\n",
    "    store.put(\n",
    "        namespace,\n",
    "        key=str(mem_id),\n",
    "        value={  # Store memory with its type\n",
    "            \"memory_type\": str(mem_type),\n",
    "            \"memory_data\": parsed_response.model_dump().get(\"memory\"),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return {\"memories\": state.get(\"memories\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_node(state: InputState) -> OverallState:\n",
    "    user_query = state.get(\"query\")\n",
    "    # Return the result as a response\n",
    "    return {\"messages\": [HumanMessage(content=user_query)], \"query\": \"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle User Query Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are Study Buddy AI, an intelligent assistant designed to support students with their study plans, research, and problem-solving tasks. Your core capabilities include:\n",
    "\n",
    "1. **Tool Assistance**: Utilize external tools such as Study Scheduler, math solvers, calendar APIs, Web Search, and knowledge bases to provide accurate, actionable, and tailored solutions.\n",
    "2. **Web Search**: Use external tools like Tavily Web Search to find relevant reference materials for the user.\n",
    "3. **Human-in-the-Loop (HITL)**: Whenever you need to get human input or clarification, always use the **ask human** tool. Make sure to gather all necessary details in a single request to avoid multiple interactions. Avoid assumptions and ask for precise, unambiguous information.\n",
    "4. **Personalized Support**: Use memory to provide tailored responses, such as recalling exam dates, preferred study times, and previous topics covered.\n",
    "5. **Agentic RAG (Retrieval-Augmented Generation)**: Retrieve contextually relevant information from knowledge bases, user documents and other tools to generate accurate, concise answers.\n",
    "\n",
    "### BEHAVIORAL GUIDELINES:\n",
    "- Be polite, clear, and concise in your responses.\n",
    "- Ensure accuracy and ask for clarification if needed to avoid misunderstandings.\n",
    "- Provide detailed explanations, especially when it comes to study concepts or solving problems.\n",
    "- Always provide educational insights to help the user understand, not just the answer.\n",
    "- When planning or scheduling, ask about topics to cover, time availability, and any constraints or preferences related to the study routine using the **ask human** tool.\n",
    "- If the user has no preferences or details, create an optimal study schedule based on best practices.\n",
    "- For ambiguous or complex queries, **pause and use the ask human tool** to involve a human assistant.\n",
    "\n",
    "### REQUIREMENTS:\n",
    "- If there is missing or unclear information (e.g., study preferences, specific topics, or scheduling constraints), you must **ask human** to clarify.\n",
    "- If you need to gather multiple pieces of information, **ask human** should include all necessary details in one message.\n",
    "- ALWAYS USE THE **ask human** to take any kind of input or information from the user and always ask precise questions to avoid multiple rounds of clarification.\n",
    "- If you have already asked for user input and you still need more information you can invoke the ask human tool again.\n",
    "- DO NOT EVER DO TOOL CALLING IN PARALLEL, ALWAYS DO SEQUENTAL.\n",
    "- The content can NEVER be ONLY '\\n' either leave it empty or add someting.\n",
    "- ALWAYS FIRST RETRIEVE DOCUMENTS.\n",
    "\n",
    "### EXAMPLES:\n",
    "\n",
    "#### Human: \"What is Langgraph?\"\n",
    "- **Tool Call**: Use tools to search the web and gather content.\n",
    "- **AI**: \"Langgraph is an AI framework designed to...\"\n",
    "\n",
    "#### Human: \"Please create a study schedule for me.\"\n",
    "- **Tool Call (ask human)**: \n",
    "  - AI: \"In order to create a personalized study schedule for you, I'll need a few details. Please answer the following:\n",
    "         **Asks the user all the details needed to create a schedule.**\n",
    "- **AI (after receiving user input)**: \n",
    "  - **Tool Call**: calls the create study schedule to create schedule.\n",
    "  - AI: \"Here is your Study schedule...\"\n",
    "\n",
    "#### Human: \"Please save that schedule in a (markdown|csv) file.\"\n",
    "- **Tool Call**: Save the study schedule in (markdown|csv) format.\n",
    "- **AI (using tool message)**: \"I have successfully saved your study schedule in (markdown|csv) format. Let me know if you need any further adjustments.\"\n",
    "\n",
    "#### Human: \"What is my name?\"\n",
    "- **Tool Call**: Tries to search in documents.\n",
    "- **AI (If answer was not found in docs)**: \"I'm sorry, but I don’t know your name yet. Could you please share it with me so I can address you properly in future conversations?\"\n",
    "\n",
    "### MEMORIES\n",
    "{memories}\n",
    "\n",
    "### SUMMARY\n",
    "{summary}\n",
    "\n",
    "System Time: {time}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def handle_user_query_node(state: OverallState) -> OverallState:\n",
    "    \"\"\"\n",
    "    Calls the model with the correct input state, integrating all messages (including tool calls and prior responses)\n",
    "    to ensure the model has the full context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the conversation history and user query\n",
    "    messages = state.get(\"messages\", [])\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    memories = state.get(\"memories\", \"\")\n",
    "\n",
    "    # Construct the system message with the summary and memories (if available)\n",
    "    system_message = SystemMessage(\n",
    "        content=system_prompt.format(\n",
    "            summary=summary,  # Include summary context if available\n",
    "            memories=memories,  # Include memory context if available\n",
    "            time=datetime.now().isoformat(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Combine system message, prior tool messages\n",
    "    # Ensure all relevant messages are included in the context\n",
    "    all_messages = [system_message] + messages\n",
    "\n",
    "    # Call the model with the full message history (system message, tool messages, and user query)\n",
    "    result = model_with_tools.invoke(all_messages)\n",
    "\n",
    "    if result.tool_calls:\n",
    "        result = AIMessage(**result.model_dump(exclude=[\"content\"]), content=\"\")\n",
    "\n",
    "    # Return the result as a response\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarization Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization_node(state: OverallState) -> OverallState:\n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\")\n",
    "    messages = state.get(\"messages\")\n",
    "\n",
    "    # Create our summarization prompt\n",
    "    if summary:\n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    if len(messages) > 6:\n",
    "        # Add prompt to our history\n",
    "        messages = messages + [HumanMessage(content=summary_message)]\n",
    "        response = model.invoke(messages)\n",
    "        # Delete all but the 2 most recent messages\n",
    "        delete_messages = [RemoveMessage(id=m.id or \"\") for m in messages[:-2]]\n",
    "\n",
    "        return {\n",
    "            \"summary\": response.content,\n",
    "            \"messages\": delete_messages,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        return {\"summary\": \"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_node(state: OverallState) -> OutputState:\n",
    "    return {\"response\": state[\"messages\"][-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT COMPILATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "workflow = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"input\", input_node)\n",
    "workflow.add_node(\"handle_user_query\", handle_user_query_node)\n",
    "workflow.add_node(\"get_memory\", get_memory_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"summarization\", summarization_node)\n",
    "workflow.add_node(\"finalize_output\", output_node)\n",
    "# workflow.add_node(\"update_memory\", update_memory_node)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"input\")\n",
    "workflow.add_edge(\"input\", \"get_memory\")\n",
    "workflow.add_edge(\"get_memory\", \"handle_user_query\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_user_query\", tools_condition, {\"tools\": \"tools\", END: END}\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"handle_user_query\")\n",
    "workflow.add_edge(\"handle_user_query\", \"summarization\")\n",
    "workflow.add_edge(\"summarization\", \"finalize_output\")\n",
    "# workflow.add_edge(\"update_memory\", \"finalize_output\")\n",
    "workflow.add_edge(\"finalize_output\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile(checkpointer=checkpointer, store=in_memory_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(agent.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING SCENARIOIS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEED REFERENCE MATERIALS FOR LEARNING?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.tools[0].function_declarations[7].parameters.properties[v__kwargs].properties: should be non-empty for OBJECT type\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1134\u001b[0m, in \u001b[0;36mGenerativeServiceClient.stream_generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 1134\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[0;32m   1135\u001b[0m     request,\n\u001b[0;32m   1136\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[0;32m   1137\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1138\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m   1139\u001b[0m )\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    294\u001b[0m     target,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    296\u001b[0m     sleep_generator,\n\u001b[0;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     _retry_error_helper(\n\u001b[0;32m    154\u001b[0m         exc,\n\u001b[0;32m    155\u001b[0m         deadline,\n\u001b[0;32m    156\u001b[0m         sleep,\n\u001b[0;32m    157\u001b[0m         error_list,\n\u001b[0;32m    158\u001b[0m         predicate,\n\u001b[0;32m    159\u001b[0m         on_error,\n\u001b[0;32m    160\u001b[0m         exception_factory,\n\u001b[0;32m    161\u001b[0m         timeout,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:174\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 * GenerateContentRequest.tools[0].function_declarations[7].parameters.properties[v__kwargs].properties: should be non-empty for OBJECT type\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[436], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m313\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(user_id)}}\n\u001b[1;32m----> 3\u001b[0m stream_agent(\n\u001b[0;32m      4\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m      5\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m      6\u001b[0m     input_value\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI have given you a pdf about langchain, Can you tell me how large it is?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      7\u001b[0m )\n",
      "Cell \u001b[1;32mIn[413], line 45\u001b[0m, in \u001b[0;36mstream_agent\u001b[1;34m(agent, input_value, config, debug)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mA function to interact with an agent in a streaming manner, handling updates and messages\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03mduring the stream. The function facilitates communication with the agent by providing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m          It doesn't return a value but may perform side effects such as displaying output.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stream_mode, data \u001b[38;5;129;01min\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39minput_value,\n\u001b[0;32m     47\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     48\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     49\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m     50\u001b[0m ):\n\u001b[0;32m     51\u001b[0m     interrupt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     52\u001b[0m         data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__interrupt__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m interrupt \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(interrupt) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1656\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1657\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   1658\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   1659\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   1660\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   1661\u001b[0m         ):\n\u001b[0;32m   1662\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\runner.py:239\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    237\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m _panic_or_proceed(\n\u001b[0;32m    240\u001b[0m     done_futures\u001b[38;5;241m.\u001b[39munion(f \u001b[38;5;28;01mfor\u001b[39;00m f, t \u001b[38;5;129;01min\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    241\u001b[0m     panic\u001b[38;5;241m=\u001b[39mreraise,\n\u001b[0;32m    242\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\runner.py:539\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m panic:\n\u001b[1;32m--> 539\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\executor.py:76\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         task\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[431], line 87\u001b[0m, in \u001b[0;36mhandle_user_query_node\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     84\u001b[0m all_messages \u001b[38;5;241m=\u001b[39m [system_message] \u001b[38;5;241m+\u001b[39m messages\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Call the model with the full message history (system message, tool messages, and user query)\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m result \u001b[38;5;241m=\u001b[39m model_with_tools\u001b[38;5;241m.\u001b[39minvoke(all_messages)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtool_calls:\n\u001b[0;32m     90\u001b[0m     result \u001b[38;5;241m=\u001b[39m AIMessage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult\u001b[38;5;241m.\u001b[39mmodel_dump(exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]), content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5355\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5356\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5358\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    634\u001b[0m                 m,\n\u001b[0;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:839\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream(\n\u001b[0;32m    834\u001b[0m     async_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    835\u001b[0m     run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    837\u001b[0m ):\n\u001b[0;32m    838\u001b[0m     chunks: \u001b[38;5;28mlist\u001b[39m[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 839\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    840\u001b[0m         chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mresponse_metadata \u001b[38;5;241m=\u001b[39m _gen_info_and_msg_metadata(chunk)\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1029\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._stream\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream\u001b[39m(\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1005\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ChatGenerationChunk]:\n\u001b[0;32m   1018\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m   1019\u001b[0m         messages,\n\u001b[0;32m   1020\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m   1028\u001b[0m     )\n\u001b[1;32m-> 1029\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m   1030\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   1031\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mstream_generate_content,\n\u001b[0;32m   1032\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1033\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m   1034\u001b[0m     )\n\u001b[0;32m   1036\u001b[0m     prev_usage_metadata: UsageMetadata \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m action(retry_state)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: rs\u001b[38;5;241m.\u001b[39moutcome\u001b[38;5;241m.\u001b[39mresult())\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moosa\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:190\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.tools[0].function_declarations[7].parameters.properties[v__kwargs].properties: should be non-empty for OBJECT type\n",
      "\u001b[0mDuring task with name 'handle_user_query' and id '9e59fe37-896e-593d-6f61-83d951256f24'"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"313\", \"user_id\": str(user_id)}}\n",
    "\n",
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"I have given you a pdf about langchain, Can you tell me how large it is?\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '',\n",
       " 'messages': [HumanMessage(content='I have given you a pdf about langchain, Can you tell me how large it is?', additional_kwargs={}, response_metadata={}, id='cdd8e477-ed8c-406b-a4c5-6fe2153dc86b')],\n",
       " 'memories': ''}"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_state(config).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I found a few resources that should help you get started with Langgraph:\n",
       "\n",
       "1.  **LangGraph Quickstart**: This tutorial from the official Langchain documentation guides you through building a chatbot that uses tools and keeps track of conversation history. It also covers human-in-the-loop capabilities and time-travel features.\n",
       "    [https://langchain-ai.github.io/langgraph/tutorials/](https://langchain-ai.github.io/langgraph/tutorials/)\n",
       "2.  **Introduction to LangGraph: A Beginner's Guide**: This article provides an overview of LangGraph's core concepts and includes simple examples to help beginners understand how to use it for building stateful applications with LLMs.\n",
       "    [https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141](https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141)\n",
       "3.  **Getting Started with LangGraph: A Practical Guide for Beginners**: This guide explains the basics of LangGraph, why it's useful, and how to set it up.\n",
       "    [https://toxigon.com/getting-started-with-langgraph](https://toxigon.com/getting-started-with-langgraph)\n",
       "\n",
       "These resources should provide a good starting point for learning Langgraph. Let me know if you have any other questions or need further assistance!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\n",
    "        \"query\": \"Hi, I am trying to Learn Langgraph, can you please find some reference material for me to get Started?\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING PERSISTANCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You previously asked me to find some reference material to help you get started with learning Langgraph.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"Please Tell me what i said before\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING MEMORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Hi Moosa, it's great to hear from you again! I remember you mentioned that you love Langgraph. Is there anything specific you'd like to discuss or work on today related to Langgraph or any other topic?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"Hi, I am Moosa and i love langgraph\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"111\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I don't have access to that information. Could you please share your name with me?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"What is my Name?\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING STUDY SCHEDULE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"12\", \"user_id\": str(user_id)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I will create a study schedule for you based on your preferences. You have a Python test on Friday morning, you are available to study in the mornings, and you want to study for at most 3 hours each day. You want to cover all major topics in Python and prefer at least half-hour breaks between study sessions. You also want to include all days until the test from now.\n",
       "\n",
       "Since today is December 28th, 2024, I will create a schedule for today, December 28th, 29th, 30th, 31st, and January 1st.\n",
       "\n",
       "To make the schedule effective, I will allocate 3 hours each day with half-hour breaks between sessions. I will also include a mix of focused study and review sessions.\n",
       "\n",
       "Here is the study schedule:\n",
       "\n",
       "**Saturday, December 28th:**\n",
       "*   **9:00 AM - 10:00 AM:** Focused Study - Python Basics (Data types, variables, operators)\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Focused Study - Control Structures (if, for, while loops)\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Review - Python Basics and Control Structures\n",
       "\n",
       "**Sunday, December 29th:**\n",
       "*   **9:00 AM - 10:00 AM:** Focused Study - Functions and Modules\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Focused Study - Working with Files\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Review - Functions, Modules, and File Handling\n",
       "\n",
       "**Monday, December 30th:**\n",
       "*   **9:00 AM - 10:00 AM:** Focused Study - Object-Oriented Programming (Classes, Objects)\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Focused Study - Exception Handling\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Review - OOP and Exception Handling\n",
       "\n",
       "**Tuesday, December 31st:**\n",
       "*   **9:00 AM - 10:00 AM:** Focused Study - Working with Libraries (e.g., NumPy, Pandas)\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Focused Study - Data Structures (Lists, Dictionaries, Tuples)\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Review - Libraries and Data Structures\n",
       "\n",
       "**Wednesday, January 1st:**\n",
       "*   **9:00 AM - 10:00 AM:** Practice - Solving coding problems\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Review - All topics\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Practice - Mock test\n",
       "\n",
       "**Thursday, January 2nd:**\n",
       "*   **9:00 AM - 10:00 AM:** Final Review - Quick review of all topics\n",
       "*   **10:00 AM - 10:30 AM:** Break\n",
       "*   **10:30 AM - 11:30 AM:** Practice - Mock test\n",
       "*   **11:30 AM - 12:00 PM:** Break\n",
       "*   **12:00 PM - 1:00 PM:** Relax and prepare for the test\n",
       "\n",
       "I will now save this schedule to a markdown file.\n",
       "\n",
       "Okay, I've created a study schedule for you, focusing on major Python topics and including breaks. The schedule is saved in `study_schedule.md`. Let me know if you need any adjustments!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\n",
    "        \"query\": \"Hi, I Like python very much and have a python test upcoming on friday, please create a study schedule for me\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '',\n",
       " 'response': AIMessage(content=\"Okay, I've created a study schedule for you, focusing on major Python topics and including breaks. The schedule is saved in `study_schedule.md`. Let me know if you need any adjustments!\\n\", additional_kwargs={}, response_metadata={'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}], 'finish_reason': 'STOP'}, id='run-6e1f9f20-f1ce-4c3e-be18-62179be6a6cd', usage_metadata={'input_tokens': 5086, 'output_tokens': 43, 'total_tokens': 5129, 'input_token_details': {'cache_read': 0}}),\n",
       " 'messages': [HumanMessage(content='Hi, I Like python very much and have a python test upcoming on friday, please create a study schedule for me', additional_kwargs={}, response_metadata={}, id='21972f0b-ce06-49b3-b654-77ec254aac77'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'ask_human', 'arguments': '{\"questions\": [{\"question\": \"What time is your python test on Friday?\"}, {\"question\": \"What time are you available to study each day?\"}, {\"question\": \"How many hours do you want to study each day?\"}, {\"question\": \"What specific topics in Python do you want to focus on for your test?\"}, {\"question\": \"Do you have any specific preferences for your study sessions, such as study type (focused, review, practice) or priority of topics?\"}]}'}}, response_metadata={'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}], 'finish_reason': 'STOP'}, id='run-fdd8599a-ef9c-420c-b988-b72b66d47f60', tool_calls=[{'name': 'ask_human', 'args': {'questions': [{'question': 'What time is your python test on Friday?'}, {'question': 'What time are you available to study each day?'}, {'question': 'How many hours do you want to study each day?'}, {'question': 'What specific topics in Python do you want to focus on for your test?'}, {'question': 'Do you have any specific preferences for your study sessions, such as study type (focused, review, practice) or priority of topics?'}]}, 'id': '643afd7d-6620-4865-8dc8-825b31db2c55', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3748, 'output_tokens': 81, 'total_tokens': 3829, 'input_token_details': {'cache_read': 0}}),\n",
       "  ToolMessage(content='[{\"id\": \"e547a4c3-91ef-4adc-b63a-06bd5cc10c53\", \"question\": \"What time is your python test on Friday?\", \"answer\": \"morning\"}, {\"id\": \"98966947-e8d1-4bee-a0c8-11afd0dd8e1c\", \"question\": \"What time are you available to study each day?\", \"answer\": \"mornings\"}, {\"id\": \"91c2d0fc-8393-4398-9c38-48dc0f60531a\", \"question\": \"How many hours do you want to study each day?\", \"answer\": \"at most 3\"}, {\"id\": \"4ef556f6-b32e-4e6a-a6f4-7ef9ff1c9d7d\", \"question\": \"What specific topics in Python do you want to focus on for your test?\", \"answer\": \"all major ones\"}, {\"id\": \"ff0935e5-f2f2-4916-9528-71088b93db03\", \"question\": \"Do you have any specific preferences for your study sessions, such as study type (focused, review, practice) or priority of topics?\", \"answer\": \"at least half hour breaks please and include all days until test from now\"}]', name='ask_human', id='3784701b-2758-4950-babb-57a33d6e1ae1', tool_call_id='643afd7d-6620-4865-8dc8-825b31db2c55'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'save_study_schedule_to_markdown', 'arguments': '{\"markdown\": \"\\\\n**Saturday, December 28th:**\\\\n*   **9:00 AM - 10:00 AM:** Focused Study - Python Basics (Data types, variables, operators)\\\\n*   **10:00 AM - 10:30 AM:** Break\\\\n*   **10:30 AM - 11:30 AM:** Focused Study - Control Structures (if, for, while loops)\\\\n*   **11:30 AM - 12:00 PM:** Break\\\\n*   **12:00 PM - 1:00 PM:** Review - Python Basics and Control Structures\\\\n\\\\n**Sunday, December 29th:**\\\\n*   **9:00 AM - 10:00 AM:** Focused Study - Functions and Modules\\\\n*   **10:00 AM - 10:30 AM:** Break\\\\n*   **10:30 AM - 11:30 AM:** Focused Study - Working with Files\\\\n*   **11:30 AM - 12:00 PM:** Break\\\\n*   **12:00 PM - 1:00 PM:** Review - Functions, Modules, and File Handling\\\\n\\\\n**Monday, December 30th:**\\\\n*   **9:00 AM - 10:00 AM:** Focused Study - Object-Oriented Programming (Classes, Objects)\\\\n*   **10:00 AM - 10:30 AM:** Break\\\\n*   **10:30 AM - 11:30 AM:** Focused Study - Exception Handling\\\\n*   **11:30 AM - 12:00 PM:** Break\\\\n*   **12:00 PM - 1:00 PM:** Review - OOP and Exception Handling\\\\n\\\\n**Tuesday, December 31st:**\\\\n*   **9:00 AM - 10:00 AM:** Focused Study - Working with Libraries (e.g., NumPy, Pandas)\\\\n*   **10:00 AM - 10:30 AM:** Break\\\\n*   **10:30 AM - 11:30 AM:** Focused Study - Data Structures (Lists, Dictionaries, Tuples)\\\\n*   **11:30 AM - 12:00 PM:** Break\\\\n*   **12:00 PM - 1:00 PM:** Review - Libraries and Data Structures\\\\n\\\\n**Wednesday, January 1st:**\\\\n*   **9:00 AM - 10:00 AM:** Practice - Solving coding problems\\\\n*   **10:00 AM - 10:30 AM:** Break\\\\n*   **10:30 AM - 11:30 AM:** Review - All topics\\\\n*   **11:30 AM - 12:00 PM:** Break\\\\n*   **12:00 PM - 1:00 PM:** Practice - Mock test\\\\n\\\\n**Thursday, January 2nd:**\\\\n*   **9:00 AM - 10:00 AM:** Final Review - Quick review of all topics\\\\n*   **10:00 AM - 10:30 AM:** Break\\\\n*   **10:30 AM - 11:30 AM:** Practice - Mock test\\\\n*   **11:30 AM - 12:00 PM:** Break\\\\n*   **12:00 PM - 1:00 PM:** Relax and prepare for the test\\\\n\", \"file_name\": \"study_schedule.md\"}'}}, response_metadata={'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}], 'finish_reason': 'STOP'}, id='run-e393f538-d434-46a3-a136-f6bf3c4c42c4', tool_calls=[{'name': 'save_study_schedule_to_markdown', 'args': {'markdown': '\\n**Saturday, December 28th:**\\n*   **9:00 AM - 10:00 AM:** Focused Study - Python Basics (Data types, variables, operators)\\n*   **10:00 AM - 10:30 AM:** Break\\n*   **10:30 AM - 11:30 AM:** Focused Study - Control Structures (if, for, while loops)\\n*   **11:30 AM - 12:00 PM:** Break\\n*   **12:00 PM - 1:00 PM:** Review - Python Basics and Control Structures\\n\\n**Sunday, December 29th:**\\n*   **9:00 AM - 10:00 AM:** Focused Study - Functions and Modules\\n*   **10:00 AM - 10:30 AM:** Break\\n*   **10:30 AM - 11:30 AM:** Focused Study - Working with Files\\n*   **11:30 AM - 12:00 PM:** Break\\n*   **12:00 PM - 1:00 PM:** Review - Functions, Modules, and File Handling\\n\\n**Monday, December 30th:**\\n*   **9:00 AM - 10:00 AM:** Focused Study - Object-Oriented Programming (Classes, Objects)\\n*   **10:00 AM - 10:30 AM:** Break\\n*   **10:30 AM - 11:30 AM:** Focused Study - Exception Handling\\n*   **11:30 AM - 12:00 PM:** Break\\n*   **12:00 PM - 1:00 PM:** Review - OOP and Exception Handling\\n\\n**Tuesday, December 31st:**\\n*   **9:00 AM - 10:00 AM:** Focused Study - Working with Libraries (e.g., NumPy, Pandas)\\n*   **10:00 AM - 10:30 AM:** Break\\n*   **10:30 AM - 11:30 AM:** Focused Study - Data Structures (Lists, Dictionaries, Tuples)\\n*   **11:30 AM - 12:00 PM:** Break\\n*   **12:00 PM - 1:00 PM:** Review - Libraries and Data Structures\\n\\n**Wednesday, January 1st:**\\n*   **9:00 AM - 10:00 AM:** Practice - Solving coding problems\\n*   **10:00 AM - 10:30 AM:** Break\\n*   **10:30 AM - 11:30 AM:** Review - All topics\\n*   **11:30 AM - 12:00 PM:** Break\\n*   **12:00 PM - 1:00 PM:** Practice - Mock test\\n\\n**Thursday, January 2nd:**\\n*   **9:00 AM - 10:00 AM:** Final Review - Quick review of all topics\\n*   **10:00 AM - 10:30 AM:** Break\\n*   **10:30 AM - 11:30 AM:** Practice - Mock test\\n*   **11:30 AM - 12:00 PM:** Break\\n*   **12:00 PM - 1:00 PM:** Relax and prepare for the test\\n', 'file_name': 'study_schedule.md'}, 'id': 'ee9f36b3-eb0b-4252-b7ed-562ea7388bab', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4210, 'output_tokens': 1750, 'total_tokens': 5960, 'input_token_details': {'cache_read': 0}}),\n",
       "  ToolMessage(content='Study schedule saved successfully to study_schedule.md', name='save_study_schedule_to_markdown', id='e13018d2-0653-4d1c-b84b-dc5fd57c3900', tool_call_id='ee9f36b3-eb0b-4252-b7ed-562ea7388bab'),\n",
       "  AIMessage(content=\"Okay, I've created a study schedule for you, focusing on major Python topics and including breaks. The schedule is saved in `study_schedule.md`. Let me know if you need any adjustments!\\n\", additional_kwargs={}, response_metadata={'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}], 'finish_reason': 'STOP'}, id='run-6e1f9f20-f1ce-4c3e-be18-62179be6a6cd', usage_metadata={'input_tokens': 5086, 'output_tokens': 43, 'total_tokens': 5129, 'input_token_details': {'cache_read': 0}})],\n",
       " 'summary': '',\n",
       " 'memories': ''}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_state(config).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I have already saved the schedule in a markdown file named `study_schedule.md`. Is there anything else I can help you with?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_agent(\n",
    "    agent=agent,\n",
    "    config=config,\n",
    "    input_value={\"query\": \"Thats Great! Please save the schedule in a markdown file\"},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
